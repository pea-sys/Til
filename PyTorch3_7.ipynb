{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch3-7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMJLd+onJU5bki5yyegG5hV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pea-sys/Til/blob/master/PyTorch3_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV-AuFy3keLO",
        "colab_type": "text"
      },
      "source": [
        "# 3.7 学習と検証の実施\n",
        "本ファイルでは、PSPNetの学習と検証の実施を行います。\n",
        "\n",
        "# 学習目標\n",
        "* PSPNetの学習と検証を実装できるようになる\n",
        "* セマンティックセグメンテーションのファインチューニングを理解する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oF4bOQOkZML",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9a2ac231-42d6-4e98-8171-f3b9a8781f10"
      },
      "source": [
        "!git clone https://github.com/YutaroOgawa/pytorch_advanced.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'pytorch_advanced' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxyR3bVNp8vk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d7cb8c08-791a-438e-a255-087f4b96309e"
      },
      "source": [
        "%cd pytorch_advanced/\n",
        "%cd 3_semantic_segmentation/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/pytorch_advanced\n",
            "/content/pytorch_advanced/3_semantic_segmentation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UZsVRE5p962",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import tarfile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4eo81Ajp_vV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# フォルダ「data」が存在しない場合は作成する\n",
        "data_dir = \"./data/\"\n",
        "if not os.path.exists(data_dir):\n",
        "    os.mkdir(data_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNCzIkkMqBQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# フォルダ「weights」が存在しない場合は作成する\n",
        "weights_dir = \"./weights/\"\n",
        "if not os.path.exists(weights_dir):\n",
        "    os.mkdir(weights_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5QX1uwCqDm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# VOC2012のデータセットをここからダウンロードします\n",
        "# 時間がかかります（約15分）\n",
        "url = \"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\"\n",
        "target_path = os.path.join(data_dir, \"VOCtrainval_11-May-2012.tar\") \n",
        "\n",
        "if not os.path.exists(target_path):\n",
        "    urllib.request.urlretrieve(url, target_path)\n",
        "    \n",
        "    tar = tarfile.TarFile(target_path)  # tarファイルを読み込み\n",
        "    tar.extractall(data_dir)  # tarを解凍\n",
        "    tar.close()  # tarファイルをクローズ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajHKx5A7q-t_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "url = \"https://drive.google.com/open?id=12eN6SpnawYuQmD1k9VgVW3QSgPR6hICc\"\n",
        "target_path = os.path.join(weights_dir, \"pspnet50_ADE20K.pth\") \n",
        "\n",
        "if not os.path.exists(target_path):\n",
        "  urllib.request.urlretrieve(url, target_path)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08mtSnCsrSww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# パッケージのimport\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-09u1qirwAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# 初期設定\n",
        "# Setup seeds\n",
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "random.seed(1234)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CB_gVlhr0Ul",
        "colab_type": "text"
      },
      "source": [
        "# DataLoader作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7KeOyq4rx8f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from utils.dataloader import make_datapath_list, DataTransform, VOCDataset\n",
        "\n",
        "# ファイルパスリスト作成\n",
        "rootpath = \"./data/VOCdevkit/VOC2012/\"\n",
        "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(\n",
        "    rootpath=rootpath)\n",
        "\n",
        "# Dataset作成\n",
        "# (RGB)の色の平均値と標準偏差\n",
        "color_mean = (0.485, 0.456, 0.406)\n",
        "color_std = (0.229, 0.224, 0.225)\n",
        "\n",
        "train_dataset = VOCDataset(train_img_list, train_anno_list, phase=\"train\", transform=DataTransform(\n",
        "    input_size=475, color_mean=color_mean, color_std=color_std))\n",
        "\n",
        "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DataTransform(\n",
        "    input_size=475, color_mean=color_mean, color_std=color_std))\n",
        "\n",
        "# DataLoader作成\n",
        "batch_size = 8\n",
        "\n",
        "train_dataloader = data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataloader = data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 辞書型変数にまとめる\n",
        "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtQp3g2Cr5WT",
        "colab_type": "text"
      },
      "source": [
        "# ネットワークモデル作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Oq1GiANr3KO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cef604fc-ad31-45c6-fa27-07e5df93d0c6"
      },
      "source": [
        "\n",
        "from utils.pspnet import PSPNet\n",
        "\n",
        "# ファインチューニングでPSPNetを作成\n",
        "# ADE20Kデータセットの学習済みモデルを使用、ADE20Kはクラス数が150です\n",
        "net = PSPNet(n_classes=150)\n",
        "\n",
        "# ADE20K学習済みパラメータをロード\n",
        "state_dict = torch.load(\"./weights/pspnet50_ADE20K.pth\")\n",
        "#state_dict = torch.load(\"pspnet50_ADE20K.pth\")\n",
        "net.load_state_dict(state_dict)\n",
        "\n",
        "# 分類用の畳み込み層を、出力数21のものにつけかえる\n",
        "n_classes = 21\n",
        "net.decode_feature.classification = nn.Conv2d(\n",
        "    in_channels=512, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "net.aux.classification = nn.Conv2d(\n",
        "    in_channels=256, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "# 付け替えた畳み込み層を初期化する。活性化関数がシグモイド関数なのでXavierを使用する。\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        nn.init.xavier_normal_(m.weight.data)\n",
        "        if m.bias is not None:  # バイアス項がある場合\n",
        "            nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "\n",
        "net.decode_feature.classification.apply(weights_init)\n",
        "net.aux.classification.apply(weights_init)\n",
        "\n",
        "\n",
        "print('ネットワーク設定完了：学習済みの重みをロードしました')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ネットワーク設定完了：学習済みの重みをロードしました\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrVz0Xbsr8qP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d5843e7c-79ad-475c-fbeb-854fbecbc009"
      },
      "source": [
        "net"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PSPNet(\n",
              "  (feature_conv): FeatureMap_convolution(\n",
              "    (cbnr_1): conv2DBatchNormRelu(\n",
              "      (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (cbnr_2): conv2DBatchNormRelu(\n",
              "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (cbnr_3): conv2DBatchNormRelu(\n",
              "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (feature_res_1): ResidualBlockPSP(\n",
              "    (block1): bottleNeckPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (cb_residual): conv2DBatchNorm(\n",
              "        (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block2): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block3): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (feature_res_2): ResidualBlockPSP(\n",
              "    (block1): bottleNeckPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (cb_residual): conv2DBatchNorm(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block2): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block3): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block4): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (feature_dilated_res_1): ResidualBlockPSP(\n",
              "    (block1): bottleNeckPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (cb_residual): conv2DBatchNorm(\n",
              "        (conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block2): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block3): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block4): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block5): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block6): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (feature_dilated_res_2): ResidualBlockPSP(\n",
              "    (block1): bottleNeckPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (cb_residual): conv2DBatchNorm(\n",
              "        (conv): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block2): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (block3): bottleNeckIdentifyPSP(\n",
              "      (cbr_1): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cbr_2): conv2DBatchNormRelu(\n",
              "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (cb_3): conv2DBatchNorm(\n",
              "        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (pyramid_pooling): PyramidPooling(\n",
              "    (avpool_1): AdaptiveAvgPool2d(output_size=6)\n",
              "    (cbr_1): conv2DBatchNormRelu(\n",
              "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (avpool_2): AdaptiveAvgPool2d(output_size=3)\n",
              "    (cbr_2): conv2DBatchNormRelu(\n",
              "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (avpool_3): AdaptiveAvgPool2d(output_size=2)\n",
              "    (cbr_3): conv2DBatchNormRelu(\n",
              "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (avpool_4): AdaptiveAvgPool2d(output_size=1)\n",
              "    (cbr_4): conv2DBatchNormRelu(\n",
              "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (decode_feature): DecodePSPFeature(\n",
              "    (cbr): conv2DBatchNormRelu(\n",
              "      (conv): Conv2d(4096, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
              "    (classification): Conv2d(512, 21, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (aux): AuxiliaryPSPlayers(\n",
              "    (cbr): conv2DBatchNormRelu(\n",
              "      (conv): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
              "    (classification): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKQDsz6N6KGi",
        "colab_type": "text"
      },
      "source": [
        "# 損失関数を定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNErY4sY6DJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 損失関数の設定\n",
        "class PSPLoss(nn.Module):\n",
        "    \"\"\"PSPNetの損失関数のクラスです。\"\"\"\n",
        "\n",
        "    def __init__(self, aux_weight=0.4):\n",
        "        super(PSPLoss, self).__init__()\n",
        "        self.aux_weight = aux_weight  # aux_lossの重み\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        \"\"\"\n",
        "        損失関数の計算。\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        outputs : PSPNetの出力(tuple)\n",
        "            (output=torch.Size([num_batch, 21, 475, 475]), output_aux=torch.Size([num_batch, 21, 475, 475]))。\n",
        "\n",
        "        targets : [num_batch, 475, 4755]\n",
        "            正解のアノテーション情報\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        loss : テンソル\n",
        "            損失の値\n",
        "        \"\"\"\n",
        "\n",
        "        loss = F.cross_entropy(outputs[0], targets, reduction='mean')\n",
        "        loss_aux = F.cross_entropy(outputs[1], targets, reduction='mean')\n",
        "\n",
        "        return loss+self.aux_weight*loss_aux\n",
        "\n",
        "\n",
        "criterion = PSPLoss(aux_weight=0.4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLa9BsnX6byB",
        "colab_type": "text"
      },
      "source": [
        "# 最適化手法を設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwX1f1EE6Zv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ファインチューニングなので、学習率は小さく\n",
        "optimizer = optim.SGD([\n",
        "    {'params': net.feature_conv.parameters(), 'lr': 1e-3},\n",
        "    {'params': net.feature_res_1.parameters(), 'lr': 1e-3},\n",
        "    {'params': net.feature_res_2.parameters(), 'lr': 1e-3},\n",
        "    {'params': net.feature_dilated_res_1.parameters(), 'lr': 1e-3},\n",
        "    {'params': net.feature_dilated_res_2.parameters(), 'lr': 1e-3},\n",
        "    {'params': net.pyramid_pooling.parameters(), 'lr': 1e-3},\n",
        "    {'params': net.decode_feature.parameters(), 'lr': 1e-2},\n",
        "    {'params': net.aux.parameters(), 'lr': 1e-2},\n",
        "], momentum=0.9, weight_decay=0.0001)\n",
        "\n",
        "\n",
        "# スケジューラーの設定\n",
        "def lambda_epoch(epoch):\n",
        "    max_epoch = 30\n",
        "    return math.pow((1-epoch/max_epoch), 0.9)\n",
        "\n",
        "\n",
        "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNcUGvJT6vVm",
        "colab_type": "text"
      },
      "source": [
        "# 学習・検証を実施する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SThqq2JE6s2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# モデルを学習させる関数を作成\n",
        "\n",
        "\n",
        "def train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs):\n",
        "\n",
        "    # GPUが使えるかを確認\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"使用デバイス：\", device)\n",
        "\n",
        "    # ネットワークをGPUへ\n",
        "    net.to(device)\n",
        "\n",
        "    # ネットワークがある程度固定であれば、高速化させる\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # 画像の枚数\n",
        "    num_train_imgs = len(dataloaders_dict[\"train\"].dataset)\n",
        "    num_val_imgs = len(dataloaders_dict[\"val\"].dataset)\n",
        "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
        "\n",
        "    # イテレーションカウンタをセット\n",
        "    iteration = 1\n",
        "    logs = []\n",
        "\n",
        "    # multiple minibatch\n",
        "    batch_multiplier = 3\n",
        "\n",
        "    # epochのループ\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # 開始時刻を保存\n",
        "        t_epoch_start = time.time()\n",
        "        t_iter_start = time.time()\n",
        "        epoch_train_loss = 0.0  # epochの損失和\n",
        "        epoch_val_loss = 0.0  # epochの損失和\n",
        "\n",
        "        print('-------------')\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-------------')\n",
        "\n",
        "        # epochごとの訓練と検証のループ\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train()  # モデルを訓練モードに\n",
        "                scheduler.step()  # 最適化schedulerの更新\n",
        "                optimizer.zero_grad()\n",
        "                print('（train）')\n",
        "\n",
        "            else:\n",
        "                if((epoch+1) % 5 == 0):\n",
        "                    net.eval()   # モデルを検証モードに\n",
        "                    print('-------------')\n",
        "                    print('（val）')\n",
        "                else:\n",
        "                    # 検証は5回に1回だけ行う\n",
        "                    continue\n",
        "\n",
        "            # データローダーからminibatchずつ取り出すループ\n",
        "            count = 0  # multiple minibatch\n",
        "            for imges, anno_class_imges in dataloaders_dict[phase]:\n",
        "                # ミニバッチがサイズが1だと、バッチノーマライゼーションでエラーになるのでさける\n",
        "                if imges.size()[0] == 1:\n",
        "                    continue\n",
        "\n",
        "                # GPUが使えるならGPUにデータを送る\n",
        "                imges = imges.to(device)\n",
        "                anno_class_imges = anno_class_imges.to(device)\n",
        "\n",
        "                \n",
        "                # multiple minibatchでのパラメータの更新\n",
        "                if (phase == 'train') and (count == 0):\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "                    count = batch_multiplier\n",
        "\n",
        "                # 順伝搬（forward）計算\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = net(imges)\n",
        "                    loss = criterion(\n",
        "                        outputs, anno_class_imges.long()) / batch_multiplier\n",
        "\n",
        "                    # 訓練時はバックプロパゲーション\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()  # 勾配の計算\n",
        "                        count -= 1  # multiple minibatch\n",
        "\n",
        "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
        "                            t_iter_finish = time.time()\n",
        "                            duration = t_iter_finish - t_iter_start\n",
        "                            print('イテレーション {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\n",
        "                                iteration, loss.item()/batch_size*batch_multiplier, duration))\n",
        "                            t_iter_start = time.time()\n",
        "\n",
        "                        epoch_train_loss += loss.item() * batch_multiplier\n",
        "                        iteration += 1\n",
        "\n",
        "                    # 検証時\n",
        "                    else:\n",
        "                        epoch_val_loss += loss.item() * batch_multiplier\n",
        "\n",
        "        # epochのphaseごとのlossと正解率\n",
        "        t_epoch_finish = time.time()\n",
        "        print('-------------')\n",
        "        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(\n",
        "            epoch+1, epoch_train_loss/num_train_imgs, epoch_val_loss/num_val_imgs))\n",
        "        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
        "        t_epoch_start = time.time()\n",
        "\n",
        "        # ログを保存\n",
        "        log_epoch = {'epoch': epoch+1, 'train_loss': epoch_train_loss /\n",
        "                     num_train_imgs, 'val_loss': epoch_val_loss/num_val_imgs}\n",
        "        logs.append(log_epoch)\n",
        "        df = pd.DataFrame(logs)\n",
        "        df.to_csv(\"log_output.csv\")\n",
        "\n",
        "    # 最後のネットワークを保存する\n",
        "    torch.save(net.state_dict(), 'weights/pspnet50_' +\n",
        "               str(epoch+1) + '.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsvIekog6zDn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d75e6df8-999a-41fd-dbd4-d685ec06c43b"
      },
      "source": [
        "# 学習・検証を実行する\n",
        "num_epochs = 30\n",
        "train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs=num_epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "使用デバイス： cuda:0\n",
            "-------------\n",
            "Epoch 1/30\n",
            "-------------\n",
            "（train）\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "イテレーション 10 || Loss: 0.4094 || 10iter: 32.4758 sec.\n",
            "イテレーション 20 || Loss: 0.4654 || 10iter: 21.1737 sec.\n",
            "イテレーション 30 || Loss: 0.1039 || 10iter: 21.6857 sec.\n",
            "イテレーション 40 || Loss: 0.1552 || 10iter: 22.6918 sec.\n",
            "イテレーション 50 || Loss: 0.1440 || 10iter: 23.6820 sec.\n",
            "イテレーション 60 || Loss: 0.1377 || 10iter: 23.0588 sec.\n",
            "イテレーション 70 || Loss: 0.1178 || 10iter: 22.8783 sec.\n",
            "イテレーション 80 || Loss: 0.1726 || 10iter: 23.1012 sec.\n",
            "イテレーション 90 || Loss: 0.1290 || 10iter: 23.0692 sec.\n",
            "イテレーション 100 || Loss: 0.1024 || 10iter: 23.0738 sec.\n",
            "イテレーション 110 || Loss: 0.1710 || 10iter: 23.1561 sec.\n",
            "イテレーション 120 || Loss: 0.1235 || 10iter: 23.0742 sec.\n",
            "イテレーション 130 || Loss: 0.1006 || 10iter: 23.0693 sec.\n",
            "イテレーション 140 || Loss: 0.1960 || 10iter: 23.0165 sec.\n",
            "イテレーション 150 || Loss: 0.1723 || 10iter: 22.9813 sec.\n",
            "イテレーション 160 || Loss: 0.2213 || 10iter: 23.0714 sec.\n",
            "イテレーション 170 || Loss: 0.2434 || 10iter: 22.9686 sec.\n",
            "イテレーション 180 || Loss: 0.1019 || 10iter: 22.9581 sec.\n",
            "-------------\n",
            "epoch 1 || Epoch_TRAIN_Loss:0.1829 ||Epoch_VAL_Loss:0.0000\n",
            "timer:  467.8475 sec.\n",
            "-------------\n",
            "Epoch 2/30\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 190 || Loss: 0.0537 || 10iter: 15.2479 sec.\n",
            "イテレーション 200 || Loss: 0.0983 || 10iter: 22.8450 sec.\n",
            "イテレーション 210 || Loss: 0.0743 || 10iter: 22.7716 sec.\n",
            "イテレーション 220 || Loss: 0.0682 || 10iter: 22.7133 sec.\n",
            "イテレーション 230 || Loss: 0.0896 || 10iter: 22.6500 sec.\n",
            "イテレーション 240 || Loss: 0.0718 || 10iter: 22.8062 sec.\n",
            "イテレーション 250 || Loss: 0.1082 || 10iter: 22.7843 sec.\n",
            "イテレーション 260 || Loss: 0.0650 || 10iter: 22.7842 sec.\n",
            "イテレーション 270 || Loss: 0.0697 || 10iter: 22.7563 sec.\n",
            "イテレーション 280 || Loss: 0.1285 || 10iter: 22.6698 sec.\n",
            "イテレーション 290 || Loss: 0.0651 || 10iter: 22.7259 sec.\n",
            "イテレーション 300 || Loss: 0.0570 || 10iter: 22.8223 sec.\n",
            "イテレーション 310 || Loss: 0.1463 || 10iter: 22.7546 sec.\n",
            "イテレーション 320 || Loss: 0.0547 || 10iter: 22.8048 sec.\n",
            "イテレーション 330 || Loss: 0.1200 || 10iter: 22.7401 sec.\n",
            "イテレーション 340 || Loss: 0.0540 || 10iter: 22.7205 sec.\n",
            "イテレーション 350 || Loss: 0.0913 || 10iter: 22.7756 sec.\n",
            "イテレーション 360 || Loss: 0.0523 || 10iter: 22.7075 sec.\n",
            "-------------\n",
            "epoch 2 || Epoch_TRAIN_Loss:0.0921 ||Epoch_VAL_Loss:0.0000\n",
            "timer:  456.7721 sec.\n",
            "-------------\n",
            "Epoch 3/30\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 370 || Loss: 0.1100 || 10iter: 7.8281 sec.\n",
            "イテレーション 380 || Loss: 0.0882 || 10iter: 22.7085 sec.\n",
            "イテレーション 390 || Loss: 0.1078 || 10iter: 22.7504 sec.\n",
            "イテレーション 400 || Loss: 0.0849 || 10iter: 22.8211 sec.\n",
            "イテレーション 410 || Loss: 0.0523 || 10iter: 22.6821 sec.\n",
            "イテレーション 420 || Loss: 0.0824 || 10iter: 22.8304 sec.\n",
            "イテレーション 430 || Loss: 0.1040 || 10iter: 22.7829 sec.\n",
            "イテレーション 440 || Loss: 0.0609 || 10iter: 22.7540 sec.\n",
            "イテレーション 450 || Loss: 0.0556 || 10iter: 22.7674 sec.\n",
            "イテレーション 460 || Loss: 0.0840 || 10iter: 22.7574 sec.\n",
            "イテレーション 470 || Loss: 0.0799 || 10iter: 22.8153 sec.\n",
            "イテレーション 480 || Loss: 0.0783 || 10iter: 22.7927 sec.\n",
            "イテレーション 490 || Loss: 0.0399 || 10iter: 22.8672 sec.\n",
            "イテレーション 500 || Loss: 0.0867 || 10iter: 22.6989 sec.\n",
            "イテレーション 510 || Loss: 0.1044 || 10iter: 22.7775 sec.\n",
            "イテレーション 520 || Loss: 0.1221 || 10iter: 22.8537 sec.\n",
            "イテレーション 530 || Loss: 0.0689 || 10iter: 22.6763 sec.\n",
            "イテレーション 540 || Loss: 0.0612 || 10iter: 22.7180 sec.\n",
            "-------------\n",
            "epoch 3 || Epoch_TRAIN_Loss:0.0773 ||Epoch_VAL_Loss:0.0000\n",
            "timer:  456.9919 sec.\n",
            "-------------\n",
            "Epoch 4/30\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 550 || Loss: 0.0831 || 10iter: 0.3225 sec.\n",
            "イテレーション 560 || Loss: 0.0817 || 10iter: 22.6361 sec.\n",
            "イテレーション 570 || Loss: 0.0732 || 10iter: 22.7473 sec.\n",
            "イテレーション 580 || Loss: 0.0933 || 10iter: 22.7496 sec.\n",
            "イテレーション 590 || Loss: 0.0457 || 10iter: 22.7907 sec.\n",
            "イテレーション 600 || Loss: 0.0795 || 10iter: 22.7623 sec.\n",
            "イテレーション 610 || Loss: 0.1046 || 10iter: 22.8098 sec.\n",
            "イテレーション 620 || Loss: 0.0581 || 10iter: 22.8215 sec.\n",
            "イテレーション 630 || Loss: 0.0729 || 10iter: 22.7122 sec.\n",
            "イテレーション 640 || Loss: 0.1065 || 10iter: 22.7564 sec.\n",
            "イテレーション 650 || Loss: 0.0763 || 10iter: 22.8049 sec.\n",
            "イテレーション 660 || Loss: 0.1139 || 10iter: 22.8285 sec.\n",
            "イテレーション 670 || Loss: 0.0481 || 10iter: 22.7690 sec.\n",
            "イテレーション 680 || Loss: 0.0947 || 10iter: 22.7575 sec.\n",
            "イテレーション 690 || Loss: 0.0746 || 10iter: 22.8669 sec.\n",
            "イテレーション 700 || Loss: 0.1408 || 10iter: 22.7366 sec.\n",
            "イテレーション 710 || Loss: 0.0688 || 10iter: 22.7840 sec.\n",
            "イテレーション 720 || Loss: 0.0549 || 10iter: 22.7895 sec.\n",
            "イテレーション 730 || Loss: 0.0731 || 10iter: 22.8459 sec.\n",
            "-------------\n",
            "epoch 4 || Epoch_TRAIN_Loss:0.0702 ||Epoch_VAL_Loss:0.0000\n",
            "timer:  457.2049 sec.\n",
            "-------------\n",
            "Epoch 5/30\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 740 || Loss: 0.0858 || 10iter: 17.8828 sec.\n",
            "イテレーション 750 || Loss: 0.1025 || 10iter: 22.7993 sec.\n",
            "イテレーション 760 || Loss: 0.0809 || 10iter: 22.7960 sec.\n",
            "イテレーション 770 || Loss: 0.0416 || 10iter: 22.6628 sec.\n",
            "イテレーション 780 || Loss: 0.0528 || 10iter: 22.7296 sec.\n",
            "イテレーション 790 || Loss: 0.0642 || 10iter: 22.8867 sec.\n",
            "イテレーション 800 || Loss: 0.0663 || 10iter: 22.7473 sec.\n",
            "イテレーション 810 || Loss: 0.0597 || 10iter: 22.7613 sec.\n",
            "イテレーション 820 || Loss: 0.0796 || 10iter: 22.7027 sec.\n",
            "イテレーション 830 || Loss: 0.0450 || 10iter: 22.8744 sec.\n",
            "イテレーション 840 || Loss: 0.0762 || 10iter: 22.9098 sec.\n",
            "イテレーション 850 || Loss: 0.0534 || 10iter: 22.7819 sec.\n",
            "イテレーション 860 || Loss: 0.0492 || 10iter: 22.8724 sec.\n",
            "イテレーション 870 || Loss: 0.0551 || 10iter: 22.8304 sec.\n",
            "イテレーション 880 || Loss: 0.0734 || 10iter: 22.8050 sec.\n",
            "イテレーション 890 || Loss: 0.0364 || 10iter: 22.7936 sec.\n",
            "イテレーション 900 || Loss: 0.0431 || 10iter: 22.8007 sec.\n",
            "イテレーション 910 || Loss: 0.0612 || 10iter: 22.8409 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 5 || Epoch_TRAIN_Loss:0.0647 ||Epoch_VAL_Loss:0.0813\n",
            "timer:  598.2447 sec.\n",
            "-------------\n",
            "Epoch 6/30\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 920 || Loss: 0.1217 || 10iter: 10.2828 sec.\n",
            "イテレーション 930 || Loss: 0.0867 || 10iter: 22.7042 sec.\n",
            "イテレーション 940 || Loss: 0.0493 || 10iter: 22.7952 sec.\n",
            "イテレーション 950 || Loss: 0.0458 || 10iter: 22.7671 sec.\n",
            "イテレーション 960 || Loss: 0.0507 || 10iter: 22.7368 sec.\n",
            "イテレーション 970 || Loss: 0.0463 || 10iter: 22.7518 sec.\n",
            "イテレーション 980 || Loss: 0.0722 || 10iter: 22.7937 sec.\n",
            "イテレーション 990 || Loss: 0.0389 || 10iter: 22.7256 sec.\n",
            "イテレーション 1000 || Loss: 0.0323 || 10iter: 22.7481 sec.\n",
            "イテレーション 1010 || Loss: 0.0337 || 10iter: 22.7750 sec.\n",
            "イテレーション 1020 || Loss: 0.0554 || 10iter: 22.6878 sec.\n",
            "イテレーション 1030 || Loss: 0.0524 || 10iter: 22.8594 sec.\n",
            "イテレーション 1040 || Loss: 0.0763 || 10iter: 22.7810 sec.\n",
            "イテレーション 1050 || Loss: 0.0342 || 10iter: 22.7666 sec.\n",
            "イテレーション 1060 || Loss: 0.0812 || 10iter: 22.8027 sec.\n",
            "イテレーション 1070 || Loss: 0.0980 || 10iter: 22.7417 sec.\n",
            "イテレーション 1080 || Loss: 0.1153 || 10iter: 22.8177 sec.\n",
            "イテレーション 1090 || Loss: 0.0938 || 10iter: 22.8402 sec.\n",
            "-------------\n",
            "epoch 6 || Epoch_TRAIN_Loss:0.0617 ||Epoch_VAL_Loss:0.0000\n",
            "timer:  457.0299 sec.\n",
            "-------------\n",
            "Epoch 7/30\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 1100 || Loss: 0.0640 || 10iter: 2.7862 sec.\n",
            "イテレーション 1110 || Loss: 0.0743 || 10iter: 22.7186 sec.\n",
            "イテレーション 1120 || Loss: 0.0368 || 10iter: 22.8218 sec.\n",
            "イテレーション 1130 || Loss: 0.1301 || 10iter: 22.8053 sec.\n",
            "イテレーション 1140 || Loss: 0.0858 || 10iter: 22.7634 sec.\n",
            "イテレーション 1150 || Loss: 0.0343 || 10iter: 22.7875 sec.\n",
            "イテレーション 1160 || Loss: 0.0437 || 10iter: 22.7106 sec.\n",
            "イテレーション 1170 || Loss: 0.0632 || 10iter: 22.7959 sec.\n",
            "イテレーション 1180 || Loss: 0.0514 || 10iter: 22.7966 sec.\n",
            "イテレーション 1190 || Loss: 0.0335 || 10iter: 22.7510 sec.\n",
            "イテレーション 1200 || Loss: 0.0756 || 10iter: 22.8067 sec.\n",
            "イテレーション 1210 || Loss: 0.0687 || 10iter: 22.8519 sec.\n",
            "イテレーション 1220 || Loss: 0.0478 || 10iter: 22.7859 sec.\n",
            "イテレーション 1230 || Loss: 0.0751 || 10iter: 22.7501 sec.\n",
            "イテレーション 1240 || Loss: 0.0588 || 10iter: 22.8486 sec.\n",
            "イテレーション 1250 || Loss: 0.0525 || 10iter: 22.7723 sec.\n",
            "イテレーション 1260 || Loss: 0.0876 || 10iter: 22.8391 sec.\n",
            "イテレーション 1270 || Loss: 0.0421 || 10iter: 22.7798 sec.\n",
            "イテレーション 1280 || Loss: 0.0423 || 10iter: 22.7854 sec.\n",
            "-------------\n",
            "epoch 7 || Epoch_TRAIN_Loss:0.0599 ||Epoch_VAL_Loss:0.0000\n",
            "timer:  457.4196 sec.\n",
            "-------------\n",
            "Epoch 8/30\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 1290 || Loss: 0.0829 || 10iter: 20.3161 sec.\n",
            "イテレーション 1300 || Loss: 0.0393 || 10iter: 22.7233 sec.\n",
            "イテレーション 1310 || Loss: 0.0625 || 10iter: 22.7672 sec.\n",
            "イテレーション 1320 || Loss: 0.0700 || 10iter: 22.7603 sec.\n",
            "イテレーション 1330 || Loss: 0.0406 || 10iter: 22.7512 sec.\n",
            "イテレーション 1340 || Loss: 0.0620 || 10iter: 22.8428 sec.\n",
            "イテレーション 1350 || Loss: 0.0456 || 10iter: 22.7855 sec.\n",
            "イテレーション 1360 || Loss: 0.0440 || 10iter: 22.8451 sec.\n",
            "イテレーション 1370 || Loss: 0.0598 || 10iter: 22.7689 sec.\n",
            "イテレーション 1380 || Loss: 0.0796 || 10iter: 22.7896 sec.\n",
            "イテレーション 1390 || Loss: 0.0362 || 10iter: 22.8057 sec.\n",
            "イテレーション 1400 || Loss: 0.0394 || 10iter: 22.8413 sec.\n",
            "イテレーション 1410 || Loss: 0.0506 || 10iter: 22.7252 sec.\n",
            "イテレーション 1420 || Loss: 0.0566 || 10iter: 22.7666 sec.\n",
            "イテレーション 1430 || Loss: 0.0583 || 10iter: 22.8911 sec.\n",
            "イテレーション 1440 || Loss: 0.0580 || 10iter: 22.7953 sec.\n",
            "イテレーション 1450 || Loss: 0.0396 || 10iter: 22.8470 sec.\n",
            "イテレーション 1460 || Loss: 0.1018 || 10iter: 22.8196 sec.\n",
            "-------------\n",
            "epoch 8 || Epoch_TRAIN_Loss:0.0572 ||Epoch_VAL_Loss:0.0000\n",
            "timer:  457.5019 sec.\n",
            "-------------\n",
            "Epoch 9/30\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 1470 || Loss: 0.0298 || 10iter: 12.8546 sec.\n",
            "イテレーション 1480 || Loss: 0.0711 || 10iter: 22.7401 sec.\n",
            "イテレーション 1490 || Loss: 0.0450 || 10iter: 22.8217 sec.\n",
            "イテレーション 1500 || Loss: 0.0391 || 10iter: 22.9068 sec.\n",
            "イテレーション 1510 || Loss: 0.0617 || 10iter: 22.8085 sec.\n",
            "イテレーション 1520 || Loss: 0.0398 || 10iter: 22.7715 sec.\n",
            "イテレーション 1530 || Loss: 0.0665 || 10iter: 22.8290 sec.\n",
            "イテレーション 1540 || Loss: 0.0672 || 10iter: 22.8291 sec.\n",
            "イテレーション 1550 || Loss: 0.0426 || 10iter: 22.8534 sec.\n",
            "イテレーション 1560 || Loss: 0.0789 || 10iter: 22.9230 sec.\n",
            "イテレーション 1570 || Loss: 0.0676 || 10iter: 22.7301 sec.\n",
            "イテレーション 1580 || Loss: 0.0537 || 10iter: 22.6904 sec.\n",
            "イテレーション 1590 || Loss: 0.0966 || 10iter: 22.8159 sec.\n",
            "イテレーション 1600 || Loss: 0.0522 || 10iter: 23.0078 sec.\n",
            "イテレーション 1610 || Loss: 0.0268 || 10iter: 22.7130 sec.\n",
            "イテレーション 1620 || Loss: 0.0369 || 10iter: 22.6722 sec.\n",
            "イテレーション 1630 || Loss: 0.0462 || 10iter: 22.7948 sec.\n",
            "イテレーション 1640 || Loss: 0.0487 || 10iter: 22.8600 sec.\n",
            "-------------\n",
            "epoch 9 || Epoch_TRAIN_Loss:0.0550 ||Epoch_VAL_Loss:0.0000\n",
            "timer:  457.7899 sec.\n",
            "-------------\n",
            "Epoch 10/30\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 1650 || Loss: 0.0619 || 10iter: 5.2832 sec.\n",
            "イテレーション 1660 || Loss: 0.0495 || 10iter: 22.8761 sec.\n",
            "イテレーション 1670 || Loss: 0.0503 || 10iter: 22.7077 sec.\n",
            "イテレーション 1680 || Loss: 0.0300 || 10iter: 22.8309 sec.\n",
            "イテレーション 1690 || Loss: 0.1144 || 10iter: 22.7501 sec.\n",
            "イテレーション 1700 || Loss: 0.0424 || 10iter: 22.7544 sec.\n",
            "イテレーション 1710 || Loss: 0.0359 || 10iter: 22.8900 sec.\n",
            "イテレーション 1720 || Loss: 0.0479 || 10iter: 22.7915 sec.\n",
            "イテレーション 1730 || Loss: 0.0721 || 10iter: 22.7299 sec.\n",
            "イテレーション 1740 || Loss: 0.0417 || 10iter: 22.7706 sec.\n",
            "イテレーション 1750 || Loss: 0.0460 || 10iter: 22.8539 sec.\n",
            "イテレーション 1760 || Loss: 0.0380 || 10iter: 22.8184 sec.\n",
            "イテレーション 1770 || Loss: 0.0851 || 10iter: 22.7764 sec.\n",
            "イテレーション 1780 || Loss: 0.0705 || 10iter: 22.7714 sec.\n",
            "イテレーション 1790 || Loss: 0.0758 || 10iter: 22.7398 sec.\n",
            "イテレーション 1800 || Loss: 0.0489 || 10iter: 22.7807 sec.\n",
            "イテレーション 1810 || Loss: 0.0771 || 10iter: 22.8299 sec.\n",
            "イテレーション 1820 || Loss: 0.0962 || 10iter: 22.7895 sec.\n",
            "イテレーション 1830 || Loss: 0.0358 || 10iter: 22.7030 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 10 || Epoch_TRAIN_Loss:0.0522 ||Epoch_VAL_Loss:0.0759\n",
            "timer:  595.7187 sec.\n",
            "-------------\n",
            "Epoch 11/30\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 1840 || Loss: 0.0716 || 10iter: 22.6588 sec.\n",
            "イテレーション 1850 || Loss: 0.0604 || 10iter: 22.7675 sec.\n",
            "イテレーション 1860 || Loss: 0.0637 || 10iter: 22.8586 sec.\n",
            "イテレーション 1870 || Loss: 0.0425 || 10iter: 22.7380 sec.\n",
            "イテレーション 1880 || Loss: 0.0673 || 10iter: 22.7823 sec.\n",
            "イテレーション 1890 || Loss: 0.0453 || 10iter: 22.7585 sec.\n",
            "イテレーション 1900 || Loss: 0.0420 || 10iter: 22.7732 sec.\n",
            "イテレーション 1910 || Loss: 0.1126 || 10iter: 22.7088 sec.\n",
            "イテレーション 1920 || Loss: 0.0614 || 10iter: 22.6837 sec.\n",
            "イテレーション 1930 || Loss: 0.0447 || 10iter: 22.7786 sec.\n",
            "イテレーション 1940 || Loss: 0.0621 || 10iter: 22.7954 sec.\n",
            "イテレーション 1950 || Loss: 0.0298 || 10iter: 22.8799 sec.\n",
            "イテレーション 1960 || Loss: 0.0625 || 10iter: 22.7925 sec.\n",
            "イテレーション 1970 || Loss: 0.0537 || 10iter: 22.7566 sec.\n",
            "イテレーション 1980 || Loss: 0.0477 || 10iter: 22.7904 sec.\n",
            "イテレーション 1990 || Loss: 0.0406 || 10iter: 22.8522 sec.\n",
            "イテレーション 2000 || Loss: 0.0349 || 10iter: 22.7486 sec.\n",
            "イテレーション 2010 || Loss: 0.0546 || 10iter: 22.7446 sec.\n",
            "-------------\n",
            "epoch 11 || Epoch_TRAIN_Loss:0.0500 ||Epoch_VAL_Loss:0.0000\n",
            "timer:  457.0457 sec.\n",
            "-------------\n",
            "Epoch 12/30\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 2020 || Loss: 0.0390 || 10iter: 15.2866 sec.\n",
            "イテレーション 2030 || Loss: 0.0325 || 10iter: 22.6456 sec.\n",
            "イテレーション 2040 || Loss: 0.0237 || 10iter: 22.7355 sec.\n",
            "イテレーション 2050 || Loss: 0.0494 || 10iter: 22.7214 sec.\n",
            "イテレーション 2060 || Loss: 0.0351 || 10iter: 22.7054 sec.\n",
            "イテレーション 2070 || Loss: 0.0406 || 10iter: 22.7518 sec.\n",
            "イテレーション 2080 || Loss: 0.0381 || 10iter: 22.8162 sec.\n",
            "イテレーション 2090 || Loss: 0.0557 || 10iter: 22.8134 sec.\n",
            "イテレーション 2100 || Loss: 0.0390 || 10iter: 22.8755 sec.\n",
            "イテレーション 2110 || Loss: 0.0617 || 10iter: 22.8677 sec.\n",
            "イテレーション 2120 || Loss: 0.0707 || 10iter: 22.8708 sec.\n",
            "イテレーション 2130 || Loss: 0.0198 || 10iter: 22.7197 sec.\n",
            "イテレーション 2140 || Loss: 0.0307 || 10iter: 22.7465 sec.\n",
            "イテレーション 2150 || Loss: 0.0747 || 10iter: 22.8463 sec.\n",
            "イテレーション 2160 || Loss: 0.0246 || 10iter: 22.8619 sec.\n",
            "イテレーション 2170 || Loss: 0.0808 || 10iter: 22.8424 sec.\n",
            "イテレーション 2180 || Loss: 0.0336 || 10iter: 22.8042 sec.\n",
            "イテレーション 2190 || Loss: 0.0666 || 10iter: 22.7905 sec.\n",
            "-------------\n",
            "epoch 12 || Epoch_TRAIN_Loss:0.0475 ||Epoch_VAL_Loss:0.0000\n",
            "timer:  457.3745 sec.\n",
            "-------------\n",
            "Epoch 13/30\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 2200 || Loss: 0.0328 || 10iter: 7.7745 sec.\n",
            "イテレーション 2210 || Loss: 0.0457 || 10iter: 22.7783 sec.\n",
            "イテレーション 2220 || Loss: 0.0563 || 10iter: 22.7727 sec.\n",
            "イテレーション 2230 || Loss: 0.0278 || 10iter: 22.7870 sec.\n",
            "イテレーション 2240 || Loss: 0.0327 || 10iter: 22.8378 sec.\n",
            "イテレーション 2250 || Loss: 0.0583 || 10iter: 22.7002 sec.\n",
            "イテレーション 2260 || Loss: 0.0424 || 10iter: 22.9109 sec.\n",
            "イテレーション 2270 || Loss: 0.0740 || 10iter: 22.8543 sec.\n",
            "イテレーション 2280 || Loss: 0.0372 || 10iter: 22.7140 sec.\n",
            "イテレーション 2290 || Loss: 0.0347 || 10iter: 22.7910 sec.\n",
            "イテレーション 2300 || Loss: 0.0401 || 10iter: 22.8620 sec.\n",
            "イテレーション 2310 || Loss: 0.0527 || 10iter: 22.8332 sec.\n",
            "イテレーション 2320 || Loss: 0.0522 || 10iter: 22.8814 sec.\n",
            "イテレーション 2330 || Loss: 0.0510 || 10iter: 22.7717 sec.\n",
            "イテレーション 2340 || Loss: 0.0807 || 10iter: 22.8619 sec.\n",
            "イテレーション 2350 || Loss: 0.0633 || 10iter: 22.8783 sec.\n",
            "イテレーション 2360 || Loss: 0.0563 || 10iter: 22.7964 sec.\n",
            "イテレーション 2370 || Loss: 0.0436 || 10iter: 22.7989 sec.\n",
            "-------------\n",
            "epoch 13 || Epoch_TRAIN_Loss:0.0473 ||Epoch_VAL_Loss:0.0000\n",
            "timer:  457.6548 sec.\n",
            "-------------\n",
            "Epoch 14/30\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 2380 || Loss: 0.0429 || 10iter: 0.3379 sec.\n",
            "イテレーション 2390 || Loss: 0.0339 || 10iter: 22.7044 sec.\n",
            "イテレーション 2400 || Loss: 0.0590 || 10iter: 22.8699 sec.\n",
            "イテレーション 2410 || Loss: 0.0326 || 10iter: 22.8787 sec.\n",
            "イテレーション 2420 || Loss: 0.0257 || 10iter: 22.7920 sec.\n",
            "イテレーション 2430 || Loss: 0.0424 || 10iter: 22.8164 sec.\n",
            "イテレーション 2440 || Loss: 0.0577 || 10iter: 22.8451 sec.\n",
            "イテレーション 2450 || Loss: 0.0574 || 10iter: 22.9132 sec.\n",
            "イテレーション 2460 || Loss: 0.0476 || 10iter: 22.7964 sec.\n",
            "イテレーション 2470 || Loss: 0.0395 || 10iter: 22.7613 sec.\n",
            "イテレーション 2480 || Loss: 0.0676 || 10iter: 22.8952 sec.\n",
            "イテレーション 2490 || Loss: 0.0501 || 10iter: 22.7615 sec.\n",
            "イテレーション 2500 || Loss: 0.0328 || 10iter: 22.7940 sec.\n",
            "イテレーション 2510 || Loss: 0.0421 || 10iter: 22.8909 sec.\n",
            "イテレーション 2520 || Loss: 0.0436 || 10iter: 22.7795 sec.\n",
            "イテレーション 2530 || Loss: 0.0653 || 10iter: 22.7149 sec.\n",
            "イテレーション 2540 || Loss: 0.0197 || 10iter: 22.8718 sec.\n",
            "イテレーション 2550 || Loss: 0.0489 || 10iter: 22.7948 sec.\n",
            "イテレーション 2560 || Loss: 0.0366 || 10iter: 22.7739 sec.\n",
            "-------------\n",
            "epoch 14 || Epoch_TRAIN_Loss:0.0471 ||Epoch_VAL_Loss:0.0000\n",
            "timer:  457.8856 sec.\n",
            "-------------\n",
            "Epoch 15/30\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 2570 || Loss: 0.0402 || 10iter: 17.7894 sec.\n",
            "イテレーション 2580 || Loss: 0.0679 || 10iter: 22.8589 sec.\n",
            "イテレーション 2590 || Loss: 0.0303 || 10iter: 22.8885 sec.\n",
            "イテレーション 2600 || Loss: 0.0290 || 10iter: 22.7843 sec.\n",
            "イテレーション 2610 || Loss: 0.0456 || 10iter: 22.8000 sec.\n",
            "イテレーション 2620 || Loss: 0.0446 || 10iter: 22.8011 sec.\n",
            "イテレーション 2630 || Loss: 0.0543 || 10iter: 22.8284 sec.\n",
            "イテレーション 2640 || Loss: 0.0535 || 10iter: 22.7543 sec.\n",
            "イテレーション 2650 || Loss: 0.0238 || 10iter: 22.7583 sec.\n",
            "イテレーション 2660 || Loss: 0.0357 || 10iter: 23.0273 sec.\n",
            "イテレーション 2670 || Loss: 0.0686 || 10iter: 22.8477 sec.\n",
            "イテレーション 2680 || Loss: 0.0505 || 10iter: 22.9581 sec.\n",
            "イテレーション 2690 || Loss: 0.0337 || 10iter: 22.7477 sec.\n",
            "イテレーション 2700 || Loss: 0.0229 || 10iter: 22.9418 sec.\n",
            "イテレーション 2710 || Loss: 0.0463 || 10iter: 22.7997 sec.\n",
            "イテレーション 2720 || Loss: 0.0420 || 10iter: 22.8381 sec.\n",
            "イテレーション 2730 || Loss: 0.0394 || 10iter: 22.7638 sec.\n",
            "イテレーション 2740 || Loss: 0.0340 || 10iter: 22.7906 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 15 || Epoch_TRAIN_Loss:0.0462 ||Epoch_VAL_Loss:0.0721\n",
            "timer:  596.4428 sec.\n",
            "-------------\n",
            "Epoch 16/30\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 2750 || Loss: 0.0528 || 10iter: 10.3150 sec.\n",
            "イテレーション 2760 || Loss: 0.0436 || 10iter: 22.8120 sec.\n",
            "イテレーション 2770 || Loss: 0.0617 || 10iter: 22.7772 sec.\n",
            "イテレーション 2780 || Loss: 0.0553 || 10iter: 22.8222 sec.\n",
            "イテレーション 2790 || Loss: 0.0357 || 10iter: 22.8447 sec.\n",
            "イテレーション 2800 || Loss: 0.0324 || 10iter: 22.8024 sec.\n",
            "イテレーション 2810 || Loss: 0.0524 || 10iter: 22.8632 sec.\n",
            "イテレーション 2820 || Loss: 0.0533 || 10iter: 22.8678 sec.\n",
            "イテレーション 2830 || Loss: 0.0546 || 10iter: 22.7961 sec.\n",
            "イテレーション 2840 || Loss: 0.0312 || 10iter: 22.8181 sec.\n",
            "イテレーション 2850 || Loss: 0.0568 || 10iter: 22.8956 sec.\n",
            "イテレーション 2860 || Loss: 0.0720 || 10iter: 22.8825 sec.\n",
            "イテレーション 2870 || Loss: 0.0538 || 10iter: 22.7836 sec.\n",
            "イテレーション 2880 || Loss: 0.0403 || 10iter: 22.8471 sec.\n",
            "イテレーション 2890 || Loss: 0.0308 || 10iter: 22.8830 sec.\n",
            "イテレーション 2900 || Loss: 0.0617 || 10iter: 22.8232 sec.\n",
            "イテレーション 2910 || Loss: 0.0450 || 10iter: 22.8050 sec.\n",
            "イテレーション 2920 || Loss: 0.0564 || 10iter: 22.8203 sec.\n",
            "-------------\n",
            "epoch 16 || Epoch_TRAIN_Loss:0.0435 ||Epoch_VAL_Loss:0.0000\n",
            "timer:  458.1715 sec.\n",
            "-------------\n",
            "Epoch 17/30\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 2930 || Loss: 0.0550 || 10iter: 2.8202 sec.\n",
            "イテレーション 2940 || Loss: 0.0535 || 10iter: 22.7547 sec.\n",
            "イテレーション 2950 || Loss: 0.0326 || 10iter: 22.8674 sec.\n",
            "イテレーション 2960 || Loss: 0.0280 || 10iter: 22.8166 sec.\n",
            "イテレーション 2970 || Loss: 0.0477 || 10iter: 22.7942 sec.\n",
            "イテレーション 2980 || Loss: 0.0588 || 10iter: 22.8151 sec.\n",
            "イテレーション 2990 || Loss: 0.0430 || 10iter: 22.9813 sec.\n",
            "イテレーション 3000 || Loss: 0.0344 || 10iter: 22.8754 sec.\n",
            "イテレーション 3010 || Loss: 0.0223 || 10iter: 22.9047 sec.\n",
            "イテレーション 3020 || Loss: 0.0290 || 10iter: 22.8187 sec.\n",
            "イテレーション 3030 || Loss: 0.0336 || 10iter: 22.8849 sec.\n",
            "イテレーション 3040 || Loss: 0.0491 || 10iter: 22.8309 sec.\n",
            "イテレーション 3050 || Loss: 0.0462 || 10iter: 22.8890 sec.\n",
            "イテレーション 3060 || Loss: 0.0422 || 10iter: 22.8342 sec.\n",
            "イテレーション 3070 || Loss: 0.0244 || 10iter: 22.7809 sec.\n",
            "イテレーション 3080 || Loss: 0.0489 || 10iter: 22.8597 sec.\n",
            "イテレーション 3090 || Loss: 0.0350 || 10iter: 22.7944 sec.\n",
            "イテレーション 3100 || Loss: 0.0398 || 10iter: 22.8633 sec.\n",
            "イテレーション 3110 || Loss: 0.0432 || 10iter: 22.8056 sec.\n",
            "-------------\n",
            "epoch 17 || Epoch_TRAIN_Loss:0.0430 ||Epoch_VAL_Loss:0.0000\n",
            "timer:  458.3626 sec.\n",
            "-------------\n",
            "Epoch 18/30\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 3120 || Loss: 0.0215 || 10iter: 20.2833 sec.\n",
            "イテレーション 3130 || Loss: 0.0453 || 10iter: 22.8739 sec.\n",
            "イテレーション 3140 || Loss: 0.0537 || 10iter: 22.7570 sec.\n",
            "イテレーション 3150 || Loss: 0.0469 || 10iter: 22.8564 sec.\n",
            "イテレーション 3160 || Loss: 0.0343 || 10iter: 22.8268 sec.\n",
            "イテレーション 3170 || Loss: 0.0473 || 10iter: 22.7924 sec.\n",
            "イテレーション 3180 || Loss: 0.0242 || 10iter: 22.7640 sec.\n",
            "イテレーション 3190 || Loss: 0.0302 || 10iter: 22.7847 sec.\n",
            "イテレーション 3200 || Loss: 0.0458 || 10iter: 22.9111 sec.\n",
            "イテレーション 3210 || Loss: 0.0397 || 10iter: 22.8080 sec.\n",
            "イテレーション 3220 || Loss: 0.0410 || 10iter: 22.9538 sec.\n",
            "イテレーション 3230 || Loss: 0.0531 || 10iter: 22.9292 sec.\n",
            "イテレーション 3240 || Loss: 0.0282 || 10iter: 22.9175 sec.\n",
            "イテレーション 3250 || Loss: 0.0559 || 10iter: 22.8615 sec.\n",
            "イテレーション 3260 || Loss: 0.0563 || 10iter: 22.9369 sec.\n",
            "イテレーション 3270 || Loss: 0.0517 || 10iter: 22.8146 sec.\n",
            "イテレーション 3280 || Loss: 0.0358 || 10iter: 22.8411 sec.\n",
            "イテレーション 3290 || Loss: 0.0311 || 10iter: 22.8241 sec.\n",
            "-------------\n",
            "epoch 18 || Epoch_TRAIN_Loss:0.0428 ||Epoch_VAL_Loss:0.0000\n",
            "timer:  458.4873 sec.\n",
            "-------------\n",
            "Epoch 19/30\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 3300 || Loss: 0.0495 || 10iter: 12.8123 sec.\n",
            "イテレーション 3310 || Loss: 0.0726 || 10iter: 22.8328 sec.\n",
            "イテレーション 3320 || Loss: 0.0393 || 10iter: 22.7919 sec.\n",
            "イテレーション 3330 || Loss: 0.0770 || 10iter: 22.7971 sec.\n",
            "イテレーション 3340 || Loss: 0.0500 || 10iter: 22.7950 sec.\n",
            "イテレーション 3350 || Loss: 0.0452 || 10iter: 22.7620 sec.\n",
            "イテレーション 3360 || Loss: 0.0264 || 10iter: 22.7815 sec.\n",
            "イテレーション 3370 || Loss: 0.0339 || 10iter: 22.8892 sec.\n",
            "イテレーション 3380 || Loss: 0.0274 || 10iter: 22.9222 sec.\n",
            "イテレーション 3390 || Loss: 0.0522 || 10iter: 22.8136 sec.\n",
            "イテレーション 3400 || Loss: 0.0365 || 10iter: 22.8610 sec.\n",
            "イテレーション 3410 || Loss: 0.0259 || 10iter: 22.8397 sec.\n",
            "イテレーション 3420 || Loss: 0.0379 || 10iter: 22.8397 sec.\n",
            "イテレーション 3430 || Loss: 0.0307 || 10iter: 22.8555 sec.\n",
            "イテレーション 3440 || Loss: 0.0357 || 10iter: 22.8084 sec.\n",
            "イテレーション 3450 || Loss: 0.0923 || 10iter: 22.8464 sec.\n",
            "イテレーション 3460 || Loss: 0.0455 || 10iter: 22.8925 sec.\n",
            "イテレーション 3470 || Loss: 0.0318 || 10iter: 22.8254 sec.\n",
            "-------------\n",
            "epoch 19 || Epoch_TRAIN_Loss:0.0428 ||Epoch_VAL_Loss:0.0000\n",
            "timer:  458.0642 sec.\n",
            "-------------\n",
            "Epoch 20/30\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 3480 || Loss: 0.0524 || 10iter: 5.2905 sec.\n",
            "イテレーション 3490 || Loss: 0.0197 || 10iter: 22.7928 sec.\n",
            "イテレーション 3500 || Loss: 0.0347 || 10iter: 22.8575 sec.\n",
            "イテレーション 3510 || Loss: 0.0530 || 10iter: 22.8684 sec.\n",
            "イテレーション 3520 || Loss: 0.0397 || 10iter: 22.7816 sec.\n",
            "イテレーション 3530 || Loss: 0.0400 || 10iter: 22.6793 sec.\n",
            "イテレーション 3540 || Loss: 0.0600 || 10iter: 22.8537 sec.\n",
            "イテレーション 3550 || Loss: 0.0297 || 10iter: 22.8330 sec.\n",
            "イテレーション 3560 || Loss: 0.0308 || 10iter: 22.8041 sec.\n",
            "イテレーション 3570 || Loss: 0.0945 || 10iter: 22.9096 sec.\n",
            "イテレーション 3580 || Loss: 0.0431 || 10iter: 22.8258 sec.\n",
            "イテレーション 3590 || Loss: 0.0366 || 10iter: 22.8096 sec.\n",
            "イテレーション 3600 || Loss: 0.0731 || 10iter: 22.8859 sec.\n",
            "イテレーション 3610 || Loss: 0.0335 || 10iter: 22.9569 sec.\n",
            "イテレーション 3620 || Loss: 0.0338 || 10iter: 22.7993 sec.\n",
            "イテレーション 3630 || Loss: 0.0466 || 10iter: 22.8433 sec.\n",
            "イテレーション 3640 || Loss: 0.0286 || 10iter: 22.8249 sec.\n",
            "イテレーション 3650 || Loss: 0.0341 || 10iter: 22.8784 sec.\n",
            "イテレーション 3660 || Loss: 0.0433 || 10iter: 22.8732 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 20 || Epoch_TRAIN_Loss:0.0430 ||Epoch_VAL_Loss:0.0723\n",
            "timer:  596.9189 sec.\n",
            "-------------\n",
            "Epoch 21/30\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 3670 || Loss: 0.0311 || 10iter: 22.8021 sec.\n",
            "イテレーション 3680 || Loss: 0.0356 || 10iter: 22.8373 sec.\n",
            "イテレーション 3690 || Loss: 0.0493 || 10iter: 22.7043 sec.\n",
            "イテレーション 3700 || Loss: 0.0391 || 10iter: 22.8152 sec.\n",
            "イテレーション 3710 || Loss: 0.0290 || 10iter: 22.8672 sec.\n",
            "イテレーション 3720 || Loss: 0.0581 || 10iter: 22.8099 sec.\n",
            "イテレーション 3730 || Loss: 0.0589 || 10iter: 22.7918 sec.\n",
            "イテレーション 3740 || Loss: 0.0369 || 10iter: 22.8368 sec.\n",
            "イテレーション 3750 || Loss: 0.0411 || 10iter: 22.9087 sec.\n",
            "イテレーション 3760 || Loss: 0.0287 || 10iter: 22.9169 sec.\n",
            "イテレーション 3770 || Loss: 0.0288 || 10iter: 22.8251 sec.\n",
            "イテレーション 3780 || Loss: 0.0425 || 10iter: 22.9060 sec.\n",
            "イテレーション 3790 || Loss: 0.1235 || 10iter: 22.7690 sec.\n",
            "イテレーション 3800 || Loss: 0.0587 || 10iter: 22.9237 sec.\n",
            "イテレーション 3810 || Loss: 0.0470 || 10iter: 22.8448 sec.\n",
            "イテレーション 3820 || Loss: 0.0375 || 10iter: 22.8821 sec.\n",
            "イテレーション 3830 || Loss: 0.0408 || 10iter: 22.7857 sec.\n",
            "イテレーション 3840 || Loss: 0.0388 || 10iter: 22.8125 sec.\n",
            "-------------\n",
            "epoch 21 || Epoch_TRAIN_Loss:0.0426 ||Epoch_VAL_Loss:0.0000\n",
            "timer:  458.1213 sec.\n",
            "-------------\n",
            "Epoch 22/30\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 3850 || Loss: 0.0331 || 10iter: 15.3312 sec.\n",
            "イテレーション 3860 || Loss: 0.0684 || 10iter: 22.7625 sec.\n",
            "イテレーション 3870 || Loss: 0.0191 || 10iter: 22.8147 sec.\n",
            "イテレーション 3880 || Loss: 0.0280 || 10iter: 22.8558 sec.\n",
            "イテレーション 3890 || Loss: 0.0315 || 10iter: 22.8121 sec.\n",
            "イテレーション 3900 || Loss: 0.0289 || 10iter: 22.8497 sec.\n",
            "イテレーション 3910 || Loss: 0.0318 || 10iter: 22.8163 sec.\n",
            "イテレーション 3920 || Loss: 0.0322 || 10iter: 22.7624 sec.\n",
            "イテレーション 3930 || Loss: 0.0753 || 10iter: 22.8267 sec.\n",
            "イテレーション 3940 || Loss: 0.0341 || 10iter: 22.8445 sec.\n",
            "イテレーション 3950 || Loss: 0.0252 || 10iter: 22.8561 sec.\n",
            "イテレーション 3960 || Loss: 0.0352 || 10iter: 22.7974 sec.\n",
            "イテレーション 3970 || Loss: 0.0373 || 10iter: 22.8292 sec.\n",
            "イテレーション 3980 || Loss: 0.0265 || 10iter: 22.7565 sec.\n",
            "イテレーション 3990 || Loss: 0.0517 || 10iter: 22.7443 sec.\n",
            "イテレーション 4000 || Loss: 0.0333 || 10iter: 22.8187 sec.\n",
            "イテレーション 4010 || Loss: 0.0423 || 10iter: 22.8253 sec.\n",
            "イテレーション 4020 || Loss: 0.0382 || 10iter: 22.8482 sec.\n",
            "-------------\n",
            "epoch 22 || Epoch_TRAIN_Loss:0.0420 ||Epoch_VAL_Loss:0.0000\n",
            "timer:  457.8580 sec.\n",
            "-------------\n",
            "Epoch 23/30\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 4030 || Loss: 0.0549 || 10iter: 7.7855 sec.\n",
            "イテレーション 4040 || Loss: 0.0317 || 10iter: 22.7471 sec.\n",
            "イテレーション 4050 || Loss: 0.0584 || 10iter: 22.7784 sec.\n",
            "イテレーション 4060 || Loss: 0.0365 || 10iter: 22.7601 sec.\n",
            "イテレーション 4070 || Loss: 0.0253 || 10iter: 22.7637 sec.\n",
            "イテレーション 4080 || Loss: 0.0483 || 10iter: 22.7674 sec.\n",
            "イテレーション 4090 || Loss: 0.0307 || 10iter: 22.7903 sec.\n",
            "イテレーション 4100 || Loss: 0.0274 || 10iter: 22.6273 sec.\n",
            "イテレーション 4110 || Loss: 0.0553 || 10iter: 22.8537 sec.\n",
            "イテレーション 4120 || Loss: 0.0426 || 10iter: 22.8531 sec.\n",
            "イテレーション 4130 || Loss: 0.0458 || 10iter: 22.7843 sec.\n",
            "イテレーション 4140 || Loss: 0.0448 || 10iter: 22.8033 sec.\n",
            "イテレーション 4150 || Loss: 0.0345 || 10iter: 22.8199 sec.\n",
            "イテレーション 4160 || Loss: 0.0360 || 10iter: 22.8267 sec.\n",
            "イテレーション 4170 || Loss: 0.0416 || 10iter: 22.7915 sec.\n",
            "イテレーション 4180 || Loss: 0.0296 || 10iter: 22.8175 sec.\n",
            "イテレーション 4190 || Loss: 0.0461 || 10iter: 22.8317 sec.\n",
            "イテレーション 4200 || Loss: 0.0523 || 10iter: 22.9152 sec.\n",
            "-------------\n",
            "epoch 23 || Epoch_TRAIN_Loss:0.0421 ||Epoch_VAL_Loss:0.0000\n",
            "timer:  457.5160 sec.\n",
            "-------------\n",
            "Epoch 24/30\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 4210 || Loss: 0.0204 || 10iter: 0.2991 sec.\n",
            "イテレーション 4220 || Loss: 0.0526 || 10iter: 22.7491 sec.\n",
            "イテレーション 4230 || Loss: 0.0419 || 10iter: 22.7495 sec.\n",
            "イテレーション 4240 || Loss: 0.0529 || 10iter: 22.8178 sec.\n",
            "イテレーション 4250 || Loss: 0.0454 || 10iter: 22.8557 sec.\n",
            "イテレーション 4260 || Loss: 0.0393 || 10iter: 22.7642 sec.\n",
            "イテレーション 4270 || Loss: 0.0565 || 10iter: 22.8943 sec.\n",
            "イテレーション 4280 || Loss: 0.0414 || 10iter: 22.7506 sec.\n",
            "イテレーション 4290 || Loss: 0.0226 || 10iter: 22.8597 sec.\n",
            "イテレーション 4300 || Loss: 0.0429 || 10iter: 22.8831 sec.\n",
            "イテレーション 4310 || Loss: 0.0365 || 10iter: 22.8305 sec.\n",
            "イテレーション 4320 || Loss: 0.0345 || 10iter: 22.7961 sec.\n",
            "イテレーション 4330 || Loss: 0.0266 || 10iter: 22.8171 sec.\n",
            "イテレーション 4340 || Loss: 0.0305 || 10iter: 22.7818 sec.\n",
            "イテレーション 4350 || Loss: 0.0433 || 10iter: 22.7509 sec.\n",
            "イテレーション 4360 || Loss: 0.0475 || 10iter: 22.8217 sec.\n",
            "イテレーション 4370 || Loss: 0.0559 || 10iter: 22.8669 sec.\n",
            "イテレーション 4380 || Loss: 0.0419 || 10iter: 22.8514 sec.\n",
            "イテレーション 4390 || Loss: 0.0593 || 10iter: 22.8804 sec.\n",
            "-------------\n",
            "epoch 24 || Epoch_TRAIN_Loss:0.0422 ||Epoch_VAL_Loss:0.0000\n",
            "timer:  457.9053 sec.\n",
            "-------------\n",
            "Epoch 25/30\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 4400 || Loss: 0.0401 || 10iter: 17.8250 sec.\n",
            "イテレーション 4410 || Loss: 0.0230 || 10iter: 22.7972 sec.\n",
            "イテレーション 4420 || Loss: 0.0414 || 10iter: 22.8296 sec.\n",
            "イテレーション 4430 || Loss: 0.0386 || 10iter: 22.8449 sec.\n",
            "イテレーション 4440 || Loss: 0.0437 || 10iter: 22.8970 sec.\n",
            "イテレーション 4450 || Loss: 0.0351 || 10iter: 22.8435 sec.\n",
            "イテレーション 4460 || Loss: 0.0738 || 10iter: 22.8636 sec.\n",
            "イテレーション 4470 || Loss: 0.0234 || 10iter: 22.8410 sec.\n",
            "イテレーション 4480 || Loss: 0.0525 || 10iter: 22.8160 sec.\n",
            "イテレーション 4490 || Loss: 0.0347 || 10iter: 22.8188 sec.\n",
            "イテレーション 4500 || Loss: 0.0274 || 10iter: 22.8608 sec.\n",
            "イテレーション 4510 || Loss: 0.0544 || 10iter: 22.8664 sec.\n",
            "イテレーション 4520 || Loss: 0.0355 || 10iter: 22.8590 sec.\n",
            "イテレーション 4530 || Loss: 0.0556 || 10iter: 22.8083 sec.\n",
            "イテレーション 4540 || Loss: 0.0576 || 10iter: 22.9511 sec.\n",
            "イテレーション 4550 || Loss: 0.0387 || 10iter: 22.7957 sec.\n",
            "イテレーション 4560 || Loss: 0.0495 || 10iter: 22.7080 sec.\n",
            "イテレーション 4570 || Loss: 0.0504 || 10iter: 22.9355 sec.\n",
            "-------------\n",
            "（val）\n",
            "-------------\n",
            "epoch 25 || Epoch_TRAIN_Loss:0.0414 ||Epoch_VAL_Loss:0.0713\n",
            "timer:  596.8055 sec.\n",
            "-------------\n",
            "Epoch 26/30\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 4580 || Loss: 0.0187 || 10iter: 10.3428 sec.\n",
            "イテレーション 4590 || Loss: 0.0827 || 10iter: 22.8318 sec.\n",
            "イテレーション 4600 || Loss: 0.0682 || 10iter: 22.7711 sec.\n",
            "イテレーション 4610 || Loss: 0.0462 || 10iter: 22.8264 sec.\n",
            "イテレーション 4620 || Loss: 0.0404 || 10iter: 22.7859 sec.\n",
            "イテレーション 4630 || Loss: 0.0522 || 10iter: 22.8503 sec.\n",
            "イテレーション 4640 || Loss: 0.0205 || 10iter: 22.8089 sec.\n",
            "イテレーション 4650 || Loss: 0.0351 || 10iter: 22.7850 sec.\n",
            "イテレーション 4660 || Loss: 0.0291 || 10iter: 22.7598 sec.\n",
            "イテレーション 4670 || Loss: 0.0326 || 10iter: 22.7811 sec.\n",
            "イテレーション 4680 || Loss: 0.0620 || 10iter: 22.8584 sec.\n",
            "イテレーション 4690 || Loss: 0.0216 || 10iter: 22.7897 sec.\n",
            "イテレーション 4700 || Loss: 0.0531 || 10iter: 22.8042 sec.\n",
            "イテレーション 4710 || Loss: 0.0351 || 10iter: 22.7760 sec.\n",
            "イテレーション 4720 || Loss: 0.0358 || 10iter: 22.7782 sec.\n",
            "イテレーション 4730 || Loss: 0.0243 || 10iter: 22.7994 sec.\n",
            "イテレーション 4740 || Loss: 0.0272 || 10iter: 22.7617 sec.\n",
            "イテレーション 4750 || Loss: 0.0412 || 10iter: 22.7977 sec.\n",
            "-------------\n",
            "epoch 26 || Epoch_TRAIN_Loss:0.0409 ||Epoch_VAL_Loss:0.0000\n",
            "timer:  457.5470 sec.\n",
            "-------------\n",
            "Epoch 27/30\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 4760 || Loss: 0.0602 || 10iter: 2.8253 sec.\n",
            "イテレーション 4770 || Loss: 0.0483 || 10iter: 22.8177 sec.\n",
            "イテレーション 4780 || Loss: 0.0229 || 10iter: 22.8600 sec.\n",
            "イテレーション 4790 || Loss: 0.0305 || 10iter: 22.7781 sec.\n",
            "イテレーション 4800 || Loss: 0.0539 || 10iter: 22.7463 sec.\n",
            "イテレーション 4810 || Loss: 0.0351 || 10iter: 22.8845 sec.\n",
            "イテレーション 4820 || Loss: 0.0433 || 10iter: 22.8053 sec.\n",
            "イテレーション 4830 || Loss: 0.0385 || 10iter: 22.7136 sec.\n",
            "イテレーション 4840 || Loss: 0.0243 || 10iter: 22.8431 sec.\n",
            "イテレーション 4850 || Loss: 0.0275 || 10iter: 22.7916 sec.\n",
            "イテレーション 4860 || Loss: 0.0431 || 10iter: 22.7942 sec.\n",
            "イテレーション 4870 || Loss: 0.0324 || 10iter: 22.8250 sec.\n",
            "イテレーション 4880 || Loss: 0.0505 || 10iter: 22.7558 sec.\n",
            "イテレーション 4890 || Loss: 0.0380 || 10iter: 22.7849 sec.\n",
            "イテレーション 4900 || Loss: 0.0428 || 10iter: 22.8281 sec.\n",
            "イテレーション 4910 || Loss: 0.0421 || 10iter: 22.8293 sec.\n",
            "イテレーション 4920 || Loss: 0.0255 || 10iter: 22.7599 sec.\n",
            "イテレーション 4930 || Loss: 0.0305 || 10iter: 22.7576 sec.\n",
            "イテレーション 4940 || Loss: 0.0282 || 10iter: 22.8128 sec.\n",
            "-------------\n",
            "epoch 27 || Epoch_TRAIN_Loss:0.0393 ||Epoch_VAL_Loss:0.0000\n",
            "timer:  457.5298 sec.\n",
            "-------------\n",
            "Epoch 28/30\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 4950 || Loss: 0.0379 || 10iter: 20.3516 sec.\n",
            "イテレーション 4960 || Loss: 0.0313 || 10iter: 22.8833 sec.\n",
            "イテレーション 4970 || Loss: 0.0520 || 10iter: 22.8763 sec.\n",
            "イテレーション 4980 || Loss: 0.0327 || 10iter: 22.7999 sec.\n",
            "イテレーション 4990 || Loss: 0.0651 || 10iter: 22.8721 sec.\n",
            "イテレーション 5000 || Loss: 0.0318 || 10iter: 22.7380 sec.\n",
            "イテレーション 5010 || Loss: 0.0277 || 10iter: 22.8505 sec.\n",
            "イテレーション 5020 || Loss: 0.0451 || 10iter: 22.8959 sec.\n",
            "イテレーション 5030 || Loss: 0.0617 || 10iter: 22.8514 sec.\n",
            "イテレーション 5040 || Loss: 0.0357 || 10iter: 22.8156 sec.\n",
            "イテレーション 5050 || Loss: 0.0382 || 10iter: 22.8336 sec.\n",
            "イテレーション 5060 || Loss: 0.0346 || 10iter: 22.8526 sec.\n",
            "イテレーション 5070 || Loss: 0.0260 || 10iter: 22.8715 sec.\n",
            "イテレーション 5080 || Loss: 0.0249 || 10iter: 22.7927 sec.\n",
            "イテレーション 5090 || Loss: 0.0428 || 10iter: 22.8387 sec.\n",
            "イテレーション 5100 || Loss: 0.0299 || 10iter: 22.8713 sec.\n",
            "イテレーション 5110 || Loss: 0.0286 || 10iter: 22.8252 sec.\n",
            "イテレーション 5120 || Loss: 0.0480 || 10iter: 22.8350 sec.\n",
            "-------------\n",
            "epoch 28 || Epoch_TRAIN_Loss:0.0407 ||Epoch_VAL_Loss:0.0000\n",
            "timer:  458.3212 sec.\n",
            "-------------\n",
            "Epoch 29/30\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 5130 || Loss: 0.0590 || 10iter: 12.7722 sec.\n",
            "イテレーション 5140 || Loss: 0.0459 || 10iter: 22.8131 sec.\n",
            "イテレーション 5150 || Loss: 0.0450 || 10iter: 22.8616 sec.\n",
            "イテレーション 5160 || Loss: 0.0376 || 10iter: 22.7917 sec.\n",
            "イテレーション 5170 || Loss: 0.0444 || 10iter: 22.8737 sec.\n",
            "イテレーション 5180 || Loss: 0.0434 || 10iter: 22.9199 sec.\n",
            "イテレーション 5190 || Loss: 0.0435 || 10iter: 22.8953 sec.\n",
            "イテレーション 5200 || Loss: 0.0291 || 10iter: 22.8705 sec.\n",
            "イテレーション 5210 || Loss: 0.0396 || 10iter: 22.8778 sec.\n",
            "イテレーション 5220 || Loss: 0.0244 || 10iter: 22.8547 sec.\n",
            "イテレーション 5230 || Loss: 0.0454 || 10iter: 22.8838 sec.\n",
            "イテレーション 5240 || Loss: 0.0459 || 10iter: 22.7727 sec.\n",
            "イテレーション 5250 || Loss: 0.0333 || 10iter: 22.8860 sec.\n",
            "イテレーション 5260 || Loss: 0.0405 || 10iter: 22.9213 sec.\n",
            "イテレーション 5270 || Loss: 0.0368 || 10iter: 22.8262 sec.\n",
            "イテレーション 5280 || Loss: 0.0406 || 10iter: 22.7824 sec.\n",
            "イテレーション 5290 || Loss: 0.0313 || 10iter: 22.7644 sec.\n",
            "イテレーション 5300 || Loss: 0.0421 || 10iter: 22.8107 sec.\n",
            "-------------\n",
            "epoch 29 || Epoch_TRAIN_Loss:0.0402 ||Epoch_VAL_Loss:0.0000\n",
            "timer:  458.4051 sec.\n",
            "-------------\n",
            "Epoch 30/30\n",
            "-------------\n",
            "（train）\n",
            "イテレーション 5310 || Loss: 0.0520 || 10iter: 5.3203 sec.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJLsxABW61P-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}