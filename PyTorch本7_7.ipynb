{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch本7-7.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPOSNSFnoxS9IHfH/Da4RhE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pea-sys/Til/blob/master/PyTorch%E6%9C%AC7_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIwCqH5UQl7V",
        "colab_type": "text"
      },
      "source": [
        "# 7.7 Transformerの学習・推論、判定根拠の可視化を実装\n",
        "本ファイルでは、ここまでで作成したTransformerモデルとIMDbのDataLoaderを使用してクラス分類を学習させます。テストデータで推論をし、さらに判断根拠となるAttentionを可視化します\n",
        "\n",
        "# 7.7 学習目標\n",
        "Transformerの学習を実装できるようになる\n",
        "Transformerの判定時のAttention可視化を実装できるようになる\n",
        "\n",
        "[写経元](https://github.com/YutaroOgawa/pytorch_advanced/blob/master/7_nlp_sentiment_transformer/7-7_transformer_training_inference.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am_PtA1KSeU5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import tarfile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_iU29r2Qd2g",
        "colab_type": "code",
        "outputId": "a05986cb-cd0a-4152-ee15-bd0944b935cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "!git clone https://github.com/YutaroOgawa/pytorch_advanced.git\n",
        "%cd pytorch_advanced/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pytorch_advanced'...\n",
            "remote: Enumerating objects: 441, done.\u001b[K\n",
            "remote: Total 441 (delta 0), reused 0 (delta 0), pack-reused 441\u001b[K\n",
            "Receiving objects: 100% (441/441), 14.62 MiB | 27.17 MiB/s, done.\n",
            "Resolving deltas: 100% (231/231), done.\n",
            "/content/pytorch_advanced\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmIWan6ETJe4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# フォルダ「data」が存在しない場合は作成する\n",
        "data_dir = \"data/\"\n",
        "if not os.path.exists(data_dir):\n",
        "    os.mkdir(data_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zga3tAnKTQn1",
        "colab_type": "text"
      },
      "source": [
        "# word2vec学習済みモデルをダウンロード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7JWf5FOSlhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word2vecの日本語学習済みモデル（東北大学 乾・岡崎研究室）をダウンロード。時間が15分ほどかかります\n",
        "\n",
        "url = \"http://www.cl.ecei.tohoku.ac.jp/~m-suzuki/jawiki_vector/data/20170201.tar.bz2\"\n",
        "save_path = \"data/20170201.tar.bz2\"\n",
        "if not os.path.exists(save_path):\n",
        "    urllib.request.urlretrieve(url, save_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCY7-kNYTWZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# './data/20170201.tar.bz2'の解凍　5分ほどかかります\n",
        "\n",
        "# tarファイルを読み込み\n",
        "tar = tarfile.open('data/20170201.tar.bz2', 'r|bz2')\n",
        "tar.extractall('data/')  # 解凍\n",
        "tar.close()  # ファイルをクローズ\n",
        "\n",
        "# フォルダ「data」内にフォルダ「entity_vector」というものができ、その中に「entity_vector.model.bin」というファイルができています。"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUvehYjiTcv3",
        "colab_type": "text"
      },
      "source": [
        "# fastTextの英語学習済みモデルをダウンロード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mC_1S3McThrl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fastTextの公式の英語学習済みモデル（650MB）をダウンロード。時間が5分ほどかかります\n",
        "url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\"\n",
        "save_path = \"data/wiki-news-300d-1M.vec.zip\"\n",
        "if not os.path.exists(save_path):\n",
        "    urllib.request.urlretrieve(url, save_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9JMRlR9TjZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# フォルダ「data」内の「/wiki-news-300d-1M.vec.zip」を解凍する\n",
        "\n",
        "zip = zipfile.ZipFile(\"data/wiki-news-300d-1M.vec.zip\")\n",
        "zip.extractall(\"data/\")  # ZIPを解凍\n",
        "zip.close()  # ZIPファイルをクローズ\n",
        "\n",
        "# フォルダ「data」内にフォルダ「wiki-news-300d-1M.vec」というものができます。"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ITyoiFITm6g",
        "colab_type": "text"
      },
      "source": [
        "# IMDbデータセットをダウンロード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hzbo7nnUTn4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IMDbデータセットをダウンロード。30秒ほどでダウンロードできます\n",
        "\n",
        "url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "save_path = \"data/aclImdb_v1.tar.gz\"\n",
        "if not os.path.exists(save_path):\n",
        "    urllib.request.urlretrieve(url, save_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4YAkawVTr-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# './data/aclImdb_v1.tar.gz'の解凍　1分ほどかかります\n",
        "\n",
        "# tarファイルを読み込み\n",
        "tar = tarfile.open('data/aclImdb_v1.tar.gz')\n",
        "tar.extractall('data/')  # 解凍\n",
        "tar.close()  # ファイルをクローズ\n",
        "\n",
        "# フォルダ「data」内にフォルダ「aclImdb」というものができます。"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM81EdQzTudt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tsv形式のファイルにします\n",
        "import glob\n",
        "import os\n",
        "import io\n",
        "import string\n",
        "\n",
        "\n",
        "# 訓練データのtsvファイルを作成します\n",
        "\n",
        "f = open('data/IMDb_train.tsv', 'w')\n",
        "\n",
        "path = 'data/aclImdb/train/pos/'\n",
        "for fname in glob.glob(os.path.join(path, '*.txt')):\n",
        "    with io.open(fname, 'r', encoding=\"utf-8\") as ff:\n",
        "        text = ff.readline()\n",
        "\n",
        "        # タブがあれば消しておきます\n",
        "        text = text.replace('\\t', \" \")\n",
        "\n",
        "        text = text+'\\t'+'1'+'\\t'+'\\n'\n",
        "        f.write(text)\n",
        "\n",
        "path = 'data/aclImdb/train/neg/'\n",
        "for fname in glob.glob(os.path.join(path, '*.txt')):\n",
        "    with io.open(fname, 'r', encoding=\"utf-8\") as ff:\n",
        "        text = ff.readline()\n",
        "\n",
        "        # タブがあれば消しておきます\n",
        "        text = text.replace('\\t', \" \")\n",
        "\n",
        "        text = text+'\\t'+'0'+'\\t'+'\\n'\n",
        "        f.write(text)\n",
        "\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQIkNpD2Twbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# テストデータの作成\n",
        "\n",
        "f = open('data/IMDb_test.tsv', 'w')\n",
        "\n",
        "path = 'data/aclImdb/test/pos/'\n",
        "for fname in glob.glob(os.path.join(path, '*.txt')):\n",
        "    with io.open(fname, 'r', encoding=\"utf-8\") as ff:\n",
        "        text = ff.readline()\n",
        "\n",
        "        # タブがあれば消しておきます\n",
        "        text = text.replace('\\t', \" \")\n",
        "\n",
        "        text = text+'\\t'+'1'+'\\t'+'\\n'\n",
        "        f.write(text)\n",
        "\n",
        "\n",
        "path = 'data/aclImdb/test/neg/'\n",
        "\n",
        "for fname in glob.glob(os.path.join(path, '*.txt')):\n",
        "    with io.open(fname, 'r', encoding=\"utf-8\") as ff:\n",
        "        text = ff.readline()\n",
        "\n",
        "        # タブがあれば消しておきます\n",
        "        text = text.replace('\\t', \" \")\n",
        "\n",
        "        text = text+'\\t'+'0'+'\\t'+'\\n'\n",
        "        f.write(text)\n",
        "\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEeAP0qaTy1z",
        "colab_type": "text"
      },
      "source": [
        "# 2. 前処理と単語分割の関数を定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbW8qugKT0wh",
        "colab_type": "code",
        "outputId": "8ea3844e-19e3-4861-f34d-41a02f373b13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "# 以下の記号はスペースに置き換えます（カンマ、ピリオドを除く）。\n",
        "# punctuationとは日本語で句点という意味です\n",
        "print(\"区切り文字：\", string.punctuation)\n",
        "# !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
        "\n",
        "# 前処理\n",
        "\n",
        "\n",
        "def preprocessing_text(text):\n",
        "    # 改行コードを消去\n",
        "    text = re.sub('<br />', '', text)\n",
        "\n",
        "    # カンマ、ピリオド以外の記号をスペースに置換\n",
        "    for p in string.punctuation:\n",
        "        if (p == \".\") or (p == \",\"):\n",
        "            continue\n",
        "        else:\n",
        "            text = text.replace(p, \" \")\n",
        "\n",
        "    # ピリオドなどの前後にはスペースを入れておく\n",
        "    text = text.replace(\".\", \" . \")\n",
        "    text = text.replace(\",\", \" , \")\n",
        "    return text\n",
        "\n",
        "# 分かち書き（今回はデータが英語で、簡易的にスペースで区切る）\n",
        "\n",
        "\n",
        "def tokenizer_punctuation(text):\n",
        "    return text.strip().split()\n",
        "\n",
        "\n",
        "# 前処理と分かち書きをまとめた関数を定義\n",
        "def tokenizer_with_preprocessing(text):\n",
        "    text = preprocessing_text(text)\n",
        "    ret = tokenizer_punctuation(text)\n",
        "    return ret\n",
        "\n",
        "\n",
        "# 動作を確認します\n",
        "print(tokenizer_with_preprocessing('I like cats.'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "区切り文字： !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
            "['I', 'like', 'cats', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDTQ9nkaT7ts",
        "colab_type": "text"
      },
      "source": [
        "# DataLoaderの作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5hGHz_ZrV8V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# データを読み込んだときに、読み込んだ内容に対して行う処理を定義します\n",
        "import torchtext\n",
        "\n",
        "\n",
        "# 文章とラベルの両方に用意します\n",
        "max_length = 256\n",
        "TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer_with_preprocessing, use_vocab=True,\n",
        "                            lower=True, include_lengths=True, batch_first=True, fix_length=max_length, init_token=\"<cls>\", eos_token=\"<eos>\")\n",
        "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "# 引数の意味は次の通り\n",
        "# init_token：全部の文章で、文頭に入れておく単語\n",
        "# eos_token：全部の文章で、文末に入れておく単語"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YvQ1I3YrYQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# フォルダ「data」から各tsvファイルを読み込みます\n",
        "train_val_ds, test_ds = torchtext.data.TabularDataset.splits(\n",
        "    path='data/', train='IMDb_train.tsv',\n",
        "    test='IMDb_test.tsv', format='tsv',\n",
        "    fields=[('Text', TEXT), ('Label', LABEL)])\n",
        "\n",
        "import random\n",
        "# torchtext.data.Datasetのsplit関数で訓練データとvalidationデータを分ける\n",
        "\n",
        "train_ds, val_ds = train_val_ds.split(\n",
        "    split_ratio=0.8, random_state=random.seed(1234))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyTDq9_Vra0Y",
        "colab_type": "text"
      },
      "source": [
        "# ボキャブラリーを作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FECNSQMPrbbk",
        "colab_type": "code",
        "outputId": "54a284b8-a53f-4ed8-cc66-e8fa197f95b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# torchtextで単語ベクトルとして英語学習済みモデルを読み込みます\n",
        "\n",
        "from torchtext.vocab import Vectors\n",
        "\n",
        "english_fasttext_vectors = Vectors(name='data/wiki-news-300d-1M.vec')\n",
        "\n",
        "# ベクトル化したバージョンのボキャブラリーを作成します\n",
        "TEXT.build_vocab(train_ds, vectors=english_fasttext_vectors, min_freq=10)\n",
        "\n",
        "\n",
        "# DataLoaderを作成します（torchtextの文脈では単純にiteraterと呼ばれています）\n",
        "train_dl = torchtext.data.Iterator(train_ds, batch_size=24, train=True)\n",
        "\n",
        "val_dl = torchtext.data.Iterator(\n",
        "    val_ds, batch_size=24, train=False, sort=False)\n",
        "\n",
        "test_dl = torchtext.data.Iterator(\n",
        "    test_ds, batch_size=24, train=False, sort=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/999994 [00:00<?, ?it/s]Skipping token b'999994' with 1-dimensional vector [b'300']; likely a header\n",
            "100%|█████████▉| 999324/999994 [01:54<00:00, 8786.43it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOUq2_Xqvp18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 辞書オブジェクトにまとめる\n",
        "dataloaders_dict = {\"train\": train_dl, \"val\": val_dl}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INVhW5wNrmVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "import torchtext\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchtext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLRN4g32rp6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup seeds\n",
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "random.seed(1234)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibZuvjkirruD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embedder(nn.Module):\n",
        "    '''idで示されている単語をベクトルに変換します'''\n",
        "\n",
        "    def __init__(self, text_embedding_vectors):\n",
        "        super(Embedder, self).__init__()\n",
        "\n",
        "        self.embeddings = nn.Embedding.from_pretrained(\n",
        "            embeddings=text_embedding_vectors, freeze=True)\n",
        "        # freeze=Trueによりバックプロパゲーションで更新されず変化しなくなります\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_vec = self.embeddings(x)\n",
        "\n",
        "        return x_vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yOo5o4xruFT",
        "colab_type": "code",
        "outputId": "4926621f-f21d-4d57-9498-c21e744140d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# 動作確認\n",
        "\n",
        "# 前節のDataLoaderなどを取得\n",
        "#from utils.dataloader import get_IMDb_DataLoaders_and_TEXT\n",
        "#train_dl, val_dl, test_dl, TEXT = get_IMDb_DataLoaders_and_TEXT(\n",
        "#    max_length=256, batch_size=24)\n",
        "\n",
        "# ミニバッチの用意\n",
        "batch = next(iter(train_dl))\n",
        "\n",
        "# モデル構築\n",
        "net1 = Embedder(TEXT.vocab.vectors)\n",
        "\n",
        "# 入出力\n",
        "x = batch.Text[0]\n",
        "x1 = net1(x)  # 単語をベクトルに\n",
        "\n",
        "print(\"入力のテンソルサイズ：\", x.shape)\n",
        "print(\"出力のテンソルサイズ：\", x1.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "入力のテンソルサイズ： torch.Size([24, 256])\n",
            "出力のテンソルサイズ： torch.Size([24, 256, 300])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gmh2qaaVrw6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoder(nn.Module):\n",
        "    '''入力された単語の位置を示すベクトル情報を付加する'''\n",
        "\n",
        "    def __init__(self, d_model=300, max_seq_len=256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model  # 単語ベクトルの次元数\n",
        "\n",
        "        # 単語の順番（pos）と埋め込みベクトルの次元の位置（i）によって一意に定まる値の表をpeとして作成\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "\n",
        "        # GPUが使える場合はGPUへ送る、ここでは省略。実際に学習時には使用する\n",
        "        # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        # pe = pe.to(device)\n",
        "\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos /\n",
        "                                          (10000 ** ((2 * (i + 1))/d_model)))\n",
        "\n",
        "        # 表peの先頭に、ミニバッチ次元となる次元を足す\n",
        "        self.pe = pe.unsqueeze(0)\n",
        "\n",
        "        # 勾配を計算しないようにする\n",
        "        self.pe.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # 入力xとPositonal Encodingを足し算する\n",
        "        # xがpeよりも小さいので、大きくする\n",
        "        ret = math.sqrt(self.d_model)*x + self.pe\n",
        "        return ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve2bXFpmr2AW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    '''Transformerは本当はマルチヘッドAttentionですが、\n",
        "    分かりやすさを優先しシングルAttentionで実装します'''\n",
        "\n",
        "    def __init__(self, d_model=300):\n",
        "        super().__init__()\n",
        "\n",
        "        # SAGANでは1dConvを使用したが、今回は全結合層で特徴量を変換する\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # 出力時に使用する全結合層\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Attentionの大きさ調整の変数\n",
        "        self.d_k = d_model\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        # 全結合層で特徴量を変換\n",
        "        k = self.k_linear(k)\n",
        "        q = self.q_linear(q)\n",
        "        v = self.v_linear(v)\n",
        "\n",
        "        # Attentionの値を計算する\n",
        "        # 各値を足し算すると大きくなりすぎるので、root(d_k)で割って調整\n",
        "        weights = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # ここでmaskを計算\n",
        "        mask = mask.unsqueeze(1)\n",
        "        weights = weights.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # softmaxで規格化をする\n",
        "        normlized_weights = F.softmax(weights, dim=-1)\n",
        "\n",
        "        # AttentionをValueとかけ算\n",
        "        output = torch.matmul(normlized_weights, v)\n",
        "\n",
        "        # 全結合層で特徴量を変換\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output, normlized_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkwyPyNPr32q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff=1024, dropout=0.1):\n",
        "        '''Attention層から出力を単純に全結合層2つで特徴量を変換するだけのユニットです'''\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear_1(x)\n",
        "        x = self.dropout(F.relu(x))\n",
        "        x = self.linear_2(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGnvLmmdr6F8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # LayerNormalization層\n",
        "        # https://pytorch.org/docs/stable/nn.html?highlight=layernorm\n",
        "        self.norm_1 = nn.LayerNorm(d_model)\n",
        "        self.norm_2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Attention層\n",
        "        self.attn = Attention(d_model)\n",
        "\n",
        "        # Attentionのあとの全結合層2つ\n",
        "        self.ff = FeedForward(d_model)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # 正規化とAttention\n",
        "        x_normlized = self.norm_1(x)\n",
        "        output, normlized_weights = self.attn(\n",
        "            x_normlized, x_normlized, x_normlized, mask)\n",
        "        \n",
        "        x2 = x + self.dropout_1(output)\n",
        "\n",
        "        # 正規化と全結合層\n",
        "        x_normlized2 = self.norm_2(x2)\n",
        "        output = x2 + self.dropout_2(self.ff(x_normlized2))\n",
        "\n",
        "        return output, normlized_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xa4VQmz4r8aD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    '''Transformer_Blockの出力を使用し、最後にクラス分類させる'''\n",
        "\n",
        "    def __init__(self, d_model=300, output_dim=2):\n",
        "        super().__init__()\n",
        "\n",
        "        # 全結合層\n",
        "        self.linear = nn.Linear(d_model, output_dim)  # output_dimはポジ・ネガの2つ\n",
        "\n",
        "        # 重み初期化処理\n",
        "        nn.init.normal_(self.linear.weight, std=0.02)\n",
        "        nn.init.normal_(self.linear.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = x[:, 0, :]  # 各ミニバッチの各文の先頭の単語の特徴量（300次元）を取り出す\n",
        "        out = self.linear(x0)\n",
        "\n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6KkWCQvtWey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ClassificationHead(nn.Module):\n",
        "    '''Transformer_Blockの出力を使用し、最後にクラス分類させる'''\n",
        "\n",
        "    def __init__(self, d_model=300, output_dim=2):\n",
        "        super().__init__()\n",
        "\n",
        "        # 全結合層\n",
        "        self.linear = nn.Linear(d_model, output_dim)  # output_dimはポジ・ネガの2つ\n",
        "\n",
        "        # 重み初期化処理\n",
        "        nn.init.normal_(self.linear.weight, std=0.02)\n",
        "        nn.init.normal_(self.linear.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = x[:, 0, :]  # 各ミニバッチの各文の先頭の単語の特徴量（300次元）を取り出す\n",
        "        out = self.linear(x0)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPCHbWszsGWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 最終的なTransformerモデルのクラス\n",
        "\n",
        "\n",
        "class TransformerClassification(nn.Module):\n",
        "    '''Transformerでクラス分類させる'''\n",
        "\n",
        "    def __init__(self, text_embedding_vectors, d_model=300, max_seq_len=256, output_dim=2):\n",
        "        super().__init__()\n",
        "\n",
        "        # モデル構築\n",
        "        self.net1 = Embedder(text_embedding_vectors)\n",
        "        self.net2 = PositionalEncoder(d_model=d_model, max_seq_len=max_seq_len)\n",
        "        self.net3_1 = TransformerBlock(d_model=d_model)\n",
        "        self.net3_2 = TransformerBlock(d_model=d_model)\n",
        "        self.net4 = ClassificationHead(output_dim=output_dim, d_model=d_model)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x1 = self.net1(x)  # 単語をベクトルに\n",
        "        x2 = self.net2(x1)  # Positon情報を足し算\n",
        "        x3_1, normlized_weights_1 = self.net3_1(\n",
        "            x2, mask)  # Self-Attentionで特徴量を変換\n",
        "        x3_2, normlized_weights_2 = self.net3_2(\n",
        "            x3_1, mask)  # Self-Attentionで特徴量を変換\n",
        "        x4 = self.net4(x3_2)  # 最終出力の0単語目を使用して、分類0-1のスカラーを出力\n",
        "        return x4, normlized_weights_1, normlized_weights_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF96OmLgsLPW",
        "colab_type": "code",
        "outputId": "2433d20a-4382-4eb1-d8ff-8e358671e74c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# モデル構築\n",
        "net = TransformerClassification(\n",
        "    text_embedding_vectors=TEXT.vocab.vectors, d_model=300, max_seq_len=256, output_dim=2)\n",
        "\n",
        "# ネットワークの初期化を定義\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Linear') != -1:\n",
        "        # Liner層の初期化\n",
        "        nn.init.kaiming_normal_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "\n",
        "# 訓練モードに設定\n",
        "net.train()\n",
        "\n",
        "# TransformerBlockモジュールを初期化実行\n",
        "net.net3_1.apply(weights_init)\n",
        "net.net3_2.apply(weights_init)\n",
        "\n",
        "\n",
        "print('ネットワーク設定完了')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ネットワーク設定完了\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8DKcxj4ta2N",
        "colab_type": "text"
      },
      "source": [
        "# 損失関数と最適化手法を定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjwaEMvsteVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 損失関数の設定\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算\n",
        "\n",
        "# 最適化手法の設定\n",
        "learning_rate = 2e-5\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKlNLN7hr78a",
        "colab_type": "text"
      },
      "source": [
        "# 学習・検証を実施"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIQCDR39t0HW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# モデルを学習させる関数を作成\n",
        "\n",
        "\n",
        "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
        "\n",
        "    # GPUが使えるかを確認\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"使用デバイス：\", device)\n",
        "    print('-----start-------')\n",
        "    # ネットワークをGPUへ\n",
        "    net.to(device)\n",
        "\n",
        "    # ネットワークがある程度固定であれば、高速化させる\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # epochのループ\n",
        "    for epoch in range(num_epochs):\n",
        "        # epochごとの訓練と検証のループ\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train()  # モデルを訓練モードに\n",
        "            else:\n",
        "                net.eval()   # モデルを検証モードに\n",
        "\n",
        "            epoch_loss = 0.0  # epochの損失和\n",
        "            epoch_corrects = 0  # epochの正解数\n",
        "\n",
        "            # データローダーからミニバッチを取り出すループ\n",
        "            for batch in (dataloaders_dict[phase]):\n",
        "                # batchはTextとLableの辞書オブジェクト\n",
        "\n",
        "                # GPUが使えるならGPUにデータを送る\n",
        "                inputs = batch.Text[0].to(device)  # 文章\n",
        "                labels = batch.Label.to(device)  # ラベル\n",
        "\n",
        "                # optimizerを初期化\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # 順伝搬（forward）計算\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "                    # mask作成\n",
        "                    input_pad = 1  # 単語のIDにおいて、'<pad>': 1 なので\n",
        "                    input_mask = (inputs != input_pad)\n",
        "\n",
        "                    # Transformerに入力\n",
        "                    outputs, _, _ = net(inputs, input_mask)\n",
        "                    loss = criterion(outputs, labels)  # 損失を計算\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
        "\n",
        "                    # 訓練時はバックプロパゲーション\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                    # 結果の計算\n",
        "                    epoch_loss += loss.item() * inputs.size(0)  # lossの合計を更新\n",
        "                    # 正解数の合計を更新\n",
        "                    epoch_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # epochごとのlossと正解率\n",
        "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
        "            epoch_acc = epoch_corrects.double(\n",
        "            ) / len(dataloaders_dict[phase].dataset)\n",
        "\n",
        "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n",
        "                                                                           phase, epoch_loss, epoch_acc))\n",
        "\n",
        "    return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xqj2wZsnt26n",
        "colab_type": "code",
        "outputId": "917911c9-e3d6-4d8c-f987-5f81d4c21aef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "# 学習・検証を実行する 15分ほどかかります\n",
        "num_epochs = 10\n",
        "net_trained = train_model(net, dataloaders_dict,\n",
        "                          criterion, optimizer, num_epochs=num_epochs)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "使用デバイス： cpu\n",
            "-----start-------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|█████████▉| 999324/999994 [02:11<00:00, 8786.43it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10 | train |  Loss: 0.5436 Acc: 0.7175\n",
            "Epoch 1/10 |  val  |  Loss: 0.4215 Acc: 0.8158\n",
            "Epoch 2/10 | train |  Loss: 0.4156 Acc: 0.8134\n",
            "Epoch 2/10 |  val  |  Loss: 0.3920 Acc: 0.8322\n",
            "Epoch 3/10 | train |  Loss: 0.3834 Acc: 0.8290\n",
            "Epoch 3/10 |  val  |  Loss: 0.4010 Acc: 0.8286\n",
            "Epoch 4/10 | train |  Loss: 0.3591 Acc: 0.8415\n",
            "Epoch 4/10 |  val  |  Loss: 0.3650 Acc: 0.8420\n",
            "Epoch 5/10 | train |  Loss: 0.3513 Acc: 0.8488\n",
            "Epoch 5/10 |  val  |  Loss: 0.3451 Acc: 0.8502\n",
            "Epoch 6/10 | train |  Loss: 0.3362 Acc: 0.8538\n",
            "Epoch 6/10 |  val  |  Loss: 0.3914 Acc: 0.8336\n",
            "Epoch 7/10 | train |  Loss: 0.3293 Acc: 0.8584\n",
            "Epoch 7/10 |  val  |  Loss: 0.3508 Acc: 0.8518\n",
            "Epoch 8/10 | train |  Loss: 0.3204 Acc: 0.8617\n",
            "Epoch 8/10 |  val  |  Loss: 0.3536 Acc: 0.8488\n",
            "Epoch 9/10 | train |  Loss: 0.3134 Acc: 0.8676\n",
            "Epoch 9/10 |  val  |  Loss: 0.3552 Acc: 0.8554\n",
            "Epoch 10/10 | train |  Loss: 0.3069 Acc: 0.8691\n",
            "Epoch 10/10 |  val  |  Loss: 0.4060 Acc: 0.8292\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWwevW4j1Yg7",
        "colab_type": "text"
      },
      "source": [
        "# テストデータでの正解率を求める"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQ4duQUwt5th",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "net_trained.eval()   # モデルを検証モードに\n",
        "net_trained.to(device)\n",
        "\n",
        "epoch_corrects = 0  # epochの正解数\n",
        "\n",
        "for batch in (test_dl):  # testデータのDataLoader\n",
        "    # batchはTextとLableの辞書オブジェクト\n",
        "    \n",
        "    # GPUが使えるならGPUにデータを送る\n",
        "    inputs = batch.Text[0].to(device)  # 文章\n",
        "    labels = batch.Label.to(device)  # ラベル\n",
        "\n",
        "    # 順伝搬（forward）計算\n",
        "    with torch.set_grad_enabled(False):\n",
        "\n",
        "        # mask作成\n",
        "        input_pad = 1  # 単語のIDにおいて、'<pad>': 1 なので\n",
        "        input_mask = (inputs != input_pad)\n",
        "\n",
        "        # Transformerに入力\n",
        "        outputs, _, _ = net_trained(inputs, input_mask)\n",
        "        _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
        "\n",
        "        # 結果の計算\n",
        "        # 正解数の合計を更新\n",
        "        epoch_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "# 正解率\n",
        "epoch_acc = epoch_corrects.double() / len(test_dl.dataset)\n",
        "\n",
        "print('テストデータ{}個での正解率：{:.4f}'.format(len(test_dl.dataset),epoch_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P8sqf8b1fYn",
        "colab_type": "text"
      },
      "source": [
        "# Attentionの可視化で判定根拠を探る"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NIfjm8d1c8O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# HTMLを作成する関数を実装\n",
        "\n",
        "\n",
        "def highlight(word, attn):\n",
        "    \"Attentionの値が大きいと文字の背景が濃い赤になるhtmlを出力させる関数\"\n",
        "\n",
        "    html_color = '#%02X%02X%02X' % (\n",
        "        255, int(255*(1 - attn)), int(255*(1 - attn)))\n",
        "    return '<span style=\"background-color: {}\"> {}</span>'.format(html_color, word)\n",
        "\n",
        "\n",
        "def mk_html(index, batch, preds, normlized_weights_1, normlized_weights_2, TEXT):\n",
        "    \"HTMLデータを作成する\"\n",
        "\n",
        "    # indexの結果を抽出\n",
        "    sentence = batch.Text[0][index]  # 文章\n",
        "    label = batch.Label[index]  # ラベル\n",
        "    pred = preds[index]  # 予測\n",
        "\n",
        "    # indexのAttentionを抽出と規格化\n",
        "    attens1 = normlized_weights_1[index, 0, :]  # 0番目の<cls>のAttention\n",
        "    attens1 /= attens1.max()\n",
        "\n",
        "    attens2 = normlized_weights_2[index, 0, :]  # 0番目の<cls>のAttention\n",
        "    attens2 /= attens2.max()\n",
        "\n",
        "    # ラベルと予測結果を文字に置き換え\n",
        "    if label == 0:\n",
        "        label_str = \"Negative\"\n",
        "    else:\n",
        "        label_str = \"Positive\"\n",
        "\n",
        "    if pred == 0:\n",
        "        pred_str = \"Negative\"\n",
        "    else:\n",
        "        pred_str = \"Positive\"\n",
        "\n",
        "    # 表示用のHTMLを作成する\n",
        "    html = '正解ラベル：{}<br>推論ラベル：{}<br><br>'.format(label_str, pred_str)\n",
        "\n",
        "    # 1段目のAttention\n",
        "    html += '[TransformerBlockの1段目のAttentionを可視化]<br>'\n",
        "    for word, attn in zip(sentence, attens1):\n",
        "        html += highlight(TEXT.vocab.itos[word], attn)\n",
        "    html += \"<br><br>\"\n",
        "\n",
        "    # 2段目のAttention\n",
        "    html += '[TransformerBlockの2段目のAttentionを可視化]<br>'\n",
        "    for word, attn in zip(sentence, attens2):\n",
        "        html += highlight(TEXT.vocab.itos[word], attn)\n",
        "\n",
        "    html += \"<br><br>\"\n",
        "\n",
        "    return html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV58p_wH1mV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "# Transformerで処理\n",
        "\n",
        "# ミニバッチの用意\n",
        "batch = next(iter(test_dl))\n",
        "\n",
        "# GPUが使えるならGPUにデータを送る\n",
        "inputs = batch.Text[0].to(device)  # 文章\n",
        "labels = batch.Label.to(device)  # ラベル\n",
        "\n",
        "# mask作成\n",
        "input_pad = 1  # 単語のIDにおいて、'<pad>': 1 なので\n",
        "input_mask = (inputs != input_pad)\n",
        "\n",
        "# Transformerに入力\n",
        "outputs, normlized_weights_1, normlized_weights_2 = net_trained(\n",
        "    inputs, input_mask)\n",
        "_, preds = torch.max(outputs, 1)  # ラベルを予測\n",
        "\n",
        "\n",
        "index = 3  # 出力させたいデータ\n",
        "html_output = mk_html(index, batch, preds, normlized_weights_1,\n",
        "                      normlized_weights_2, TEXT)  # HTML作成\n",
        "HTML(html_output)  # HTML形式で出力"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIntqr3Q1qUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index = 61  # 出力させたいデータ\n",
        "html_output = mk_html(index, batch, preds, normlized_weights_1,\n",
        "                      normlized_weights_2, TEXT)  # HTML作成\n",
        "HTML(html_output)  # HTML形式で出力"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn6o9Sz-5mDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SJ4z3_V1j5V",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}